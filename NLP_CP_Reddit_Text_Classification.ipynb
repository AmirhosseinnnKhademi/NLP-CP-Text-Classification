{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "iMd5-B7wZl2Z",
        "outputId": "43307208-166d-4484-a922-dffbffb3f2df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6d542385-876b-4714-8501-5448903e8fce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6d542385-876b-4714-8501-5448903e8fce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n"
          ]
        }
      ],
      "source": [
        "#@title Import Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import nltk\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Implementation"
      ],
      "metadata": {
        "id": "8UMK08gEcFHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Zhx4oIp2WY",
        "outputId": "984d044c-9b9f-492d-ae48-fbcce391bc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32894 sha256=8f66cfeaeb740c247cadc5e8641086707b37fbd1b1e079e81507dc1fa9477870\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from stop_words import get_stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w97kM6aPo3Ij",
        "outputId": "ea4ee238-00ce-47a5-eae3-c1b6787c3e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ('to_dense', DenseTransformer()) step is included to convert sparse matrices to dense matrices because some classifiers (especially custom ones or those that expect dense input) may not work directly with sparse matrices, which are the output of TfidfVectorizer and CountVectorizer. In this case:\n",
        "\n",
        "Why it's included: Scikit-Learn’s TfidfVectorizer and CountVectorizer produce sparse matrices to save memory and computation time, especially for large text datasets. However, certain estimators, such as BernoulliNB and some custom models, may require dense input to perform their calculations.\n",
        "\n",
        "Why it wasn’t in your original code: If your original code worked without converting to dense format, it likely means that the classifiers in use (like Scikit-Learn’s BernoulliNB, DecisionTreeClassifier, and LogisticRegression) already support sparse matrices. However, when implementing custom pipelines or models, a dense transformer can sometimes be added to ensure compatibility, particularly when you’re unsure if the classifier requires dense data.\n",
        "\n",
        "When it can be removed: If you’re only using Scikit-Learn’s classifiers that support sparse matrices, you can safely remove ('to_dense', DenseTransformer()). Scikit-Learn’s BernoulliNB, DecisionTreeClassifier, and LogisticRegression all support sparse input, so you should not need the dense conversion for these classifiers."
      ],
      "metadata": {
        "id": "3iVRR72wY5Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Function to create a pipeline with vectorizer and classifier\n",
        "def create_pipeline(vectorizer, classifier):\n",
        "    return Pipeline([\n",
        "        ('lemmatize', LemmatizerTransformer()),\n",
        "        ('vectorize', vectorizer),\n",
        "        ('classify', classifier)\n",
        "    ])'''"
      ],
      "metadata": {
        "id": "H7BaW2iCY8MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_corpus = pd.read_csv('train.csv', encoding='cp720')\n",
        "test_corpus = pd.read_csv('test.csv', encoding='cp720')"
      ],
      "metadata": {
        "id": "jBjuJWhHFAQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jGuwwEx5FUIb",
        "outputId": "9def847a-4f32-4ab1-dce3-7822d1351cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                body subreddit\n",
              "0  Hello dear Redditors!  \\nMy husband and I are ...     Paris\n",
              "1  This city receives 50 millions of tourists per...     Paris\n",
              "2  En vrai la circulation en vélo commence à être...     Paris\n",
              "3  Nous avons effectué nos deux derniers déménage...     Paris\n",
              "4  If you couldn't find anything around there, I ...     Paris"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be30cdc0-140b-49c2-8ecd-18f1fde5feb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello dear Redditors!  \\nMy husband and I are ...</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This city receives 50 millions of tourists per...</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>En vrai la circulation en vélo commence à être...</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nous avons effectué nos deux derniers déménage...</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you couldn't find anything around there, I ...</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be30cdc0-140b-49c2-8ecd-18f1fde5feb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be30cdc0-140b-49c2-8ecd-18f1fde5feb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be30cdc0-140b-49c2-8ecd-18f1fde5feb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90409f33-a8c1-41c6-a0f3-7f9627a85964\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90409f33-a8c1-41c6-a0f3-7f9627a85964')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90409f33-a8c1-41c6-a0f3-7f9627a85964 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_corpus",
              "summary": "{\n  \"name\": \"train_corpus\",\n  \"rows\": 1400,\n  \"fields\": [\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1391,\n        \"samples\": [\n          \"\\\"M\\u00eame son de cloche de la part d?Ellie Israel. Selon elle, le gouvernement qu\\u00e9b\\u00e9cois vise \\u00ab\\u00e0 nous forcer de vivre et parler en fran\\u00e7ais et d?\\u00eatre oblig\\u00e9s de [nous] cacher pour parler anglais\\u00bb.\\\"\\n\\nEuhm....\",\n          \"Now I wonder how you would destroy such building in the middle of a busy city. Floor by floor, from the top to the ground? Controlled collapse?\\n\\nOr just burn Paris to the ground and go with a clean sheet, without Parisians?\",\n          \"Les banques centrales de partout au monde ont imprim\\u00e9s de l?argent de fa\\u00e7on historique pendant la COVID. Contribuant grandement eux m\\u00eame \\u00e0 l?inflation qu?ils essaient maintenant de contr\\u00f4ler en ajustant les taux d?int\\u00e9r\\u00eats. Ils ont cr\\u00e9\\u00e9 possiblement une des plus grandes r\\u00e9cessions de l?histoire \\u00e0 venir. Pas pour sonner alarmiste mais attachez vos tuques?Le pire est devant nous je crois.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subreddit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Melbourne\",\n          \"Montreal\",\n          \"Paris\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7UXvoZ4CFVgm",
        "outputId": "5b002f64-f283-4f6e-bd59-bb81f95f1b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               body\n",
              "0   0  Anyone else feel like it?s getting to a point ...\n",
              "1   1  Check out LeCypherX. Happens every last Thursd...\n",
              "2   2  'That's mine bruv' - Steal your bike back.\\n\\n...\n",
              "3   3  What might make it quite hard is that if this ...\n",
              "4   4  # Upvote/Downvote reminder\\n\\nLike this image ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a01c796-c420-4cfa-86fd-ea0a070fa599\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Anyone else feel like it?s getting to a point ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Check out LeCypherX. Happens every last Thursd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>'That's mine bruv' - Steal your bike back.\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What might make it quite hard is that if this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td># Upvote/Downvote reminder\\n\\nLike this image ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a01c796-c420-4cfa-86fd-ea0a070fa599')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a01c796-c420-4cfa-86fd-ea0a070fa599 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a01c796-c420-4cfa-86fd-ea0a070fa599');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b24c1734-f73e-4c73-8119-d363cb12029d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b24c1734-f73e-4c73-8119-d363cb12029d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b24c1734-f73e-4c73-8119-d363cb12029d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_corpus",
              "summary": "{\n  \"name\": \"test_corpus\",\n  \"rows\": 600,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 173,\n        \"min\": 0,\n        \"max\": 599,\n        \"num_unique_values\": 600,\n        \"samples\": [\n          110,\n          419,\n          565\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 595,\n        \"samples\": [\n          \"Je suis pas exactement un expert non plus, mais \\u00e7a ressemble vachement \\u00e0 une buse de Harris, un animal qui vient d'outre-atlantique.\\n\\nLa buse de Harris est l'un des tr\\u00e8s rares rapaces qui chassent en groupe et est assez populaire chez les fauconniers, donc c'est potentiellement un animal apprivois\\u00e9 ou au moins avec un propri\\u00e9taire.\",\n          \"My understanding is that e-scooters can't be used on footpaths and therefore on pedestrian crossings. \\nhttps://www.police.vic.gov.au/electric-powered-scooters-e-scooters\\nYou could consider getting legal advice, but in all likelihood, you don't have a case. Even though she's clearly a shit driver for hitting you in the first place, you were technically doing the wrong thing... I doubt you'll manage a damages claim even if you try to sue which is literally your only option.\",\n          \"There is growing evidence that covid infections damage your immune system and leave you more vulnerable to picking up other viruses.\\n\\nI am still wearing an N95 mask on public transport and in shops and I have only had one cold since 2020.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data and define train indices\n",
        "train_body = train_corpus['body']\n",
        "subreddit = train_corpus['subreddit']\n",
        "id_train = train_corpus.index  # Define train indices for tracking purposes\n",
        "id_test = test_corpus['id']\n",
        "test_body = test_corpus['body']\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Combine English and French stop words\n",
        "english_stop_words = set(stopwords.words('english'))\n",
        "french_stop_words = set(get_stop_words('french'))\n",
        "combined_stop_words = english_stop_words.union(french_stop_words)\n",
        "\n",
        "# Custom Preprocessing Transformer\n",
        "class LemmatizerTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Remove stop words and lemmatize\n",
        "        return [' '.join([lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text) if word.lower() not in combined_stop_words]) for text in X]\n",
        "\n",
        "# Transformer to convert sparse matrix to dense\n",
        "class DenseTransformer(TransformerMixin):\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.toarray()\n",
        "\n",
        "# Function to create a pipeline with vectorizer, optional DenseTransformer, SVD, and classifier\n",
        "def create_pipeline(vectorizer, svd, classifier, apply_dense_transform):\n",
        "    steps = [\n",
        "        ('lemmatize', LemmatizerTransformer()),  # Apply custom preprocessing\n",
        "        ('vectorize', vectorizer)\n",
        "    ]\n",
        "    if apply_dense_transform:\n",
        "        steps.append(('to_dense', DenseTransformer()))  # Conditionally add dense transformer\n",
        "    if svd:  # Conditionally add SVD if specified\n",
        "        steps.append(('svd', svd))\n",
        "    steps.append(('classify', classifier))\n",
        "\n",
        "    return Pipeline(steps)\n",
        "\n",
        "# Classifier options\n",
        "classifiers = {\n",
        "    'Bernoulli Naive Bayes': BernoulliNB(alpha=1e-3),  # alpha is the Laplace smoothing parameter\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'SVM': SVC(kernel='sigmoid'), # SVC(kernel='linear'), SVC(kernel='poly', degree=3), SVC(kernel='rbf', gamma='scale'), SVC(kernel='sigmoid')\n",
        "    'Random Forest': RandomForestClassifier(n_estimators = 200),\n",
        "    'XGBoost': XGBClassifier(eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "# Vectorizer options (stop words handled in preprocessing step)\n",
        "vectorizers = {\n",
        "    'TF-IDF': TfidfVectorizer(\n",
        "        min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
        "        token_pattern=r'\\w{1,}', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
        "        lowercase=True\n",
        "    ),\n",
        "    'Count Vectorizer': CountVectorizer(lowercase=True)\n",
        "}\n",
        "\n",
        "# Encode labels for XGBoost only\n",
        "label_encoder = LabelEncoder()\n",
        "subreddit_encoded = label_encoder.fit_transform(subreddit)\n",
        "\n",
        "# Adjust the evaluate_pipeline function to include timing for each fold and overall runtime\n",
        "def evaluate_pipeline(vectorizer_name, classifier_name, num_folds=5, svd_components=None, apply_dense_transform=True):\n",
        "    vectorizer = vectorizers[vectorizer_name]\n",
        "    classifier = classifiers[classifier_name]\n",
        "\n",
        "    # Optional SVD step for dimensionality reduction\n",
        "    svd = TruncatedSVD(n_components=svd_components) if svd_components else None\n",
        "\n",
        "    # Encode labels for XGBoost if classifier is XGBoost\n",
        "    y_data = subreddit_encoded if classifier_name == 'XGBoost' else subreddit\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = create_pipeline(vectorizer, svd, classifier, apply_dense_transform)\n",
        "\n",
        "    # Cross-validation with K-Fold\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "    fold_accuracies = []  # Store accuracy for each fold\n",
        "    fold_times = []  # Store runtime for each fold\n",
        "\n",
        "    print(f\"Evaluating {classifier_name} with {vectorizer_name} across {num_folds} folds...\\n\")\n",
        "\n",
        "    overall_start = time.time()  # Start time for the entire evaluation\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_body), 1):\n",
        "        fold_start = time.time()  # Start time for this fold\n",
        "\n",
        "        # Split train and validation data for the current fold\n",
        "        X_train, X_val = train_body.iloc[train_index], train_body.iloc[val_index]\n",
        "        y_train, y_val = y_data[train_index], y_data[val_index]\n",
        "\n",
        "        # Fit and evaluate the pipeline on the current fold\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        fold_accuracy = pipeline.score(X_val, y_val)\n",
        "\n",
        "        # End time for this fold and calculate fold time\n",
        "        fold_end = time.time()\n",
        "        fold_runtime = fold_end - fold_start\n",
        "        fold_times.append(fold_runtime)\n",
        "\n",
        "        # Print accuracy and runtime for the current fold and store the accuracy\n",
        "        print(f\"Fold {fold}: Accuracy = {fold_accuracy:.4f}, Runtime = {fold_runtime:.2f} seconds\")\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "    # Calculate and print average accuracy and runtime\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    avg_runtime = np.mean(fold_times)\n",
        "    overall_runtime = time.time() - overall_start\n",
        "    print(f\"\\nAverage Accuracy ({classifier_name} with {vectorizer_name}): {avg_accuracy:.4f}\")\n",
        "    print(f\"Average Fold Runtime: {avg_runtime:.2f} seconds\")\n",
        "    print(f\"Total Runtime: {overall_runtime:.2f} seconds\\n\")\n",
        "\n",
        "    # Train on the full dataset and make final predictions\n",
        "    pipeline.fit(train_body, y_data)\n",
        "    predicted_classes = pipeline.predict(test_body)\n",
        "\n",
        "    # If classifier is XGBoost, convert numeric predictions back to original text labels\n",
        "    if classifier_name == 'XGBoost':\n",
        "        predicted_classes = label_encoder.inverse_transform(predicted_classes)\n",
        "\n",
        "    # Store results in DataFrame\n",
        "    results_df = pd.DataFrame({'id': id_test, 'subreddit': predicted_classes})\n",
        "\n",
        "# Vectorizer: TF-IDF , Count Vectorizer\n",
        "# Solver:     Bernoulli Naive Bayes, Decision Tree, Logistic Regression, SVM, Random Forest, XGBoost\n",
        "\n",
        "#NOTE: Out of the models listed, only Logistic Regression and XGBoost use gradient-based optimization techniques\n",
        "\n",
        "evaluate_pipeline('Count Vectorizer', 'Bernoulli Naive Bayes', num_folds=5, svd_components=None, apply_dense_transform=True)\n",
        "#evaluate_pipeline('TF-IDF', 'SVM', num_folds=5, svd_components=600, apply_dense_transform=True)\n",
        "evaluate_pipeline('TF-IDF', 'Random Forest', num_folds=5, svd_components=None, apply_dense_transform=True)\n",
        "#evaluate_pipeline('TF-IDF', 'SVM', num_folds=5, svd_components=None, apply_dense_transform=False)\n",
        "evaluate_pipeline('TF-IDF', 'SVM', num_folds=5, svd_components=500, apply_dense_transform=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GHMnOSgU3o0",
        "outputId": "86369309-028c-45f9-cbf9-6dbb94deaaf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Bernoulli Naive Bayes with Count Vectorizer across 5 folds...\n",
            "\n",
            "Fold 1: Accuracy = 0.6071, Runtime = 2.37 seconds\n",
            "Fold 2: Accuracy = 0.6214, Runtime = 2.25 seconds\n",
            "Fold 3: Accuracy = 0.6393, Runtime = 1.73 seconds\n",
            "Fold 4: Accuracy = 0.6393, Runtime = 1.71 seconds\n",
            "Fold 5: Accuracy = 0.6321, Runtime = 1.80 seconds\n",
            "\n",
            "Average Accuracy (Bernoulli Naive Bayes with Count Vectorizer): 0.6279\n",
            "Average Fold Runtime: 1.97 seconds\n",
            "Total Runtime: 9.87 seconds\n",
            "\n",
            "Evaluating Random Forest with TF-IDF across 5 folds...\n",
            "\n",
            "Fold 1: Accuracy = 0.5964, Runtime = 7.07 seconds\n",
            "Fold 2: Accuracy = 0.6679, Runtime = 3.98 seconds\n",
            "Fold 3: Accuracy = 0.6000, Runtime = 5.18 seconds\n",
            "Fold 4: Accuracy = 0.7000, Runtime = 3.34 seconds\n",
            "Fold 5: Accuracy = 0.6107, Runtime = 3.24 seconds\n",
            "\n",
            "Average Accuracy (Random Forest with TF-IDF): 0.6350\n",
            "Average Fold Runtime: 4.56 seconds\n",
            "Total Runtime: 22.81 seconds\n",
            "\n",
            "Evaluating SVM with TF-IDF across 5 folds...\n",
            "\n",
            "Fold 1: Accuracy = 0.6714, Runtime = 5.84 seconds\n",
            "Fold 2: Accuracy = 0.6571, Runtime = 5.00 seconds\n",
            "Fold 3: Accuracy = 0.6357, Runtime = 6.99 seconds\n",
            "Fold 4: Accuracy = 0.6071, Runtime = 5.00 seconds\n",
            "Fold 5: Accuracy = 0.6821, Runtime = 6.95 seconds\n",
            "\n",
            "Average Accuracy (SVM with TF-IDF): 0.6507\n",
            "Average Fold Runtime: 5.95 seconds\n",
            "Total Runtime: 29.78 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Turning (Grid Search)"
      ],
      "metadata": {
        "id": "g4bGiAYNe-63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bernoulli Naive Bayes**"
      ],
      "metadata": {
        "id": "ZwVqB2lDfHbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define accuracy scorer using make_scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Define parameter grid for Bernoulli Naive Bayes with vectorizer, alpha, binarize, fit_prior, and dense transformation\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],  # Different vectorizers\n",
        "    'classify__alpha': [1e-6, 1e-4, 1e-2, 1e-1],  # Smoothing values for BernoulliNB\n",
        "    'classify__binarize': [0.0, 1.0],  # Threshold for binarizing\n",
        "    'classify__fit_prior': [True, False],  # Whether to learn class prior probabilities\n",
        "    'to_dense__apply_dense': [True, False]  # Optionally apply DenseTransformer\n",
        "}\n",
        "\n",
        "# Conditional step in the pipeline based on the 'apply_dense' parameter\n",
        "class ConditionalDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, apply_dense):\n",
        "        self.apply_dense = apply_dense\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return DenseTransformer().transform(X) if self.apply_dense else X\n",
        "\n",
        "# Updated pipeline with conditional dense transformation\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),  # Placeholder, updated by grid search\n",
        "    ('to_dense', ConditionalDenseTransformer(apply_dense=True)),  # Conditionally apply dense\n",
        "    ('classify', BernoulliNB())  # Bernoulli Naive Bayes classifier\n",
        "])\n",
        "\n",
        "# Grid search setup with new parameter grid, using the custom accuracy scorer\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(train_body, subreddit)\n",
        "\n",
        "# Display each combination's parameters and corresponding cross-validation results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and cross-validation score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model to predict on test_body\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Save the results\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions})\n",
        "results_df.to_csv(\"final_predictions_with_best_params.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGh9eTiyfKvm",
        "outputId": "f34f0286-18c8-43eb-91ff-fd14d7f43c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
            "[CV 1/3; 1/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 1/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   1.6s\n",
            "[CV 2/3; 1/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 1/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   1.7s\n",
            "[CV 3/3; 1/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 1/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.603 total time=   1.7s\n",
            "[CV 1/3; 2/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 2/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.621 total time=   1.9s\n",
            "[CV 2/3; 2/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 2/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.597 total time=   2.4s\n",
            "[CV 3/3; 2/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 2/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.620 total time=   2.9s\n",
            "[CV 1/3; 3/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 3/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   1.6s\n",
            "[CV 2/3; 3/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 3/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   1.5s\n",
            "[CV 3/3; 3/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 3/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.603 total time=   1.5s\n",
            "[CV 1/3; 4/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 4/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.621 total time=   1.5s\n",
            "[CV 2/3; 4/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 4/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.597 total time=   1.5s\n",
            "[CV 3/3; 4/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 4/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.620 total time=   1.5s\n",
            "[CV 1/3; 5/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 5/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   2.4s\n",
            "[CV 2/3; 5/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 5/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   2.2s\n",
            "[CV 3/3; 5/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 5/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.603 total time=   1.7s\n",
            "[CV 1/3; 6/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 6/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.621 total time=   1.9s\n",
            "[CV 2/3; 6/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 6/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.597 total time=   1.9s\n",
            "[CV 3/3; 6/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 6/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.620 total time=   1.9s\n",
            "[CV 1/3; 7/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 7/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   1.6s\n",
            "[CV 2/3; 7/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 7/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   2.5s\n",
            "[CV 3/3; 7/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 7/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.603 total time=   2.2s\n",
            "[CV 1/3; 8/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 8/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.621 total time=   1.5s\n",
            "[CV 2/3; 8/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 8/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.597 total time=   1.5s\n",
            "[CV 3/3; 8/64] START classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 8/64] END classify__alpha=1e-06, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.620 total time=   1.5s\n",
            "[CV 1/3; 9/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 9/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.6s\n",
            "[CV 2/3; 9/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 9/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.7s\n",
            "[CV 3/3; 9/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 9/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.7s\n",
            "[CV 1/3; 10/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 10/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.439 total time=   3.1s\n",
            "[CV 2/3; 10/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 10/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   2.2s\n",
            "[CV 3/3; 10/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 10/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.397 total time=   1.9s\n",
            "[CV 1/3; 11/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 11/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.9s\n",
            "[CV 2/3; 11/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 11/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.8s\n",
            "[CV 3/3; 11/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 11/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.6s\n",
            "[CV 1/3; 12/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 12/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.439 total time=   2.9s\n",
            "[CV 2/3; 12/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 12/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   3.5s\n",
            "[CV 3/3; 12/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 12/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.397 total time=   3.1s\n",
            "[CV 1/3; 13/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 13/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.9s\n",
            "[CV 2/3; 13/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 13/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.3s\n",
            "[CV 3/3; 13/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 13/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   4.8s\n",
            "[CV 1/3; 14/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 14/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.439 total time=   3.1s\n",
            "[CV 2/3; 14/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 14/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   1.9s\n",
            "[CV 3/3; 14/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 14/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.397 total time=   1.9s\n",
            "[CV 1/3; 15/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 15/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 2/3; 15/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 15/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 3/3; 15/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 15/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   2.6s\n",
            "[CV 1/3; 16/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 16/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.439 total time=   2.1s\n",
            "[CV 2/3; 16/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 16/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   1.5s\n",
            "[CV 3/3; 16/64] START classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 16/64] END classify__alpha=1e-06, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.397 total time=   1.5s\n",
            "[CV 1/3; 17/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 17/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   1.6s\n",
            "[CV 2/3; 17/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 17/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   1.7s\n",
            "[CV 3/3; 17/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 17/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.612 total time=   1.7s\n",
            "[CV 1/3; 18/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 18/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.619 total time=   2.3s\n",
            "[CV 2/3; 18/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 18/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.597 total time=   3.5s\n",
            "[CV 3/3; 18/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 18/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.631 total time=   1.9s\n",
            "[CV 1/3; 19/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 19/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   1.6s\n",
            "[CV 2/3; 19/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 19/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   1.5s\n",
            "[CV 3/3; 19/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 19/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.612 total time=   1.5s\n",
            "[CV 1/3; 20/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 20/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.619 total time=   1.5s\n",
            "[CV 2/3; 20/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 20/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.597 total time=   1.5s\n",
            "[CV 3/3; 20/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 20/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.631 total time=   2.5s\n",
            "[CV 1/3; 21/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 21/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   2.3s\n",
            "[CV 2/3; 21/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 21/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   1.7s\n",
            "[CV 3/3; 21/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 21/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.612 total time=   1.7s\n",
            "[CV 1/3; 22/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 22/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.619 total time=   1.9s\n",
            "[CV 2/3; 22/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 22/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.597 total time=   1.9s\n",
            "[CV 3/3; 22/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 22/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.631 total time=   1.9s\n",
            "[CV 1/3; 23/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 23/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.587 total time=   2.4s\n",
            "[CV 2/3; 23/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 23/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.565 total time=   2.2s\n",
            "[CV 3/3; 23/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 23/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.612 total time=   1.5s\n",
            "[CV 1/3; 24/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 24/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.619 total time=   1.5s\n",
            "[CV 2/3; 24/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 24/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.597 total time=   1.5s\n",
            "[CV 3/3; 24/64] START classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 24/64] END classify__alpha=0.0001, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.631 total time=   1.5s\n",
            "[CV 1/3; 25/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 25/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.6s\n",
            "[CV 2/3; 25/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 25/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.7s\n",
            "[CV 3/3; 25/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 25/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   2.7s\n",
            "[CV 1/3; 26/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 26/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.439 total time=   2.2s\n",
            "[CV 2/3; 26/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 26/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   1.9s\n",
            "[CV 3/3; 26/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 26/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.397 total time=   1.9s\n",
            "[CV 1/3; 27/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 27/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 2/3; 27/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 27/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 3/3; 27/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 27/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.5s\n",
            "[CV 1/3; 28/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 28/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.439 total time=   6.1s\n",
            "[CV 2/3; 28/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 28/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   1.5s\n",
            "[CV 3/3; 28/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 28/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.397 total time=   1.5s\n",
            "[CV 1/3; 29/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 29/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.4s\n",
            "[CV 2/3; 29/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 29/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.7s\n",
            "[CV 3/3; 29/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 29/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.8s\n",
            "[CV 1/3; 30/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 30/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.439 total time=   3.1s\n",
            "[CV 2/3; 30/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 30/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   2.0s\n",
            "[CV 3/3; 30/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 30/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.397 total time=   1.9s\n",
            "[CV 1/3; 31/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 31/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 2/3; 31/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 31/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 3/3; 31/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 31/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.5s\n",
            "[CV 1/3; 32/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 32/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.439 total time=   1.5s\n",
            "[CV 2/3; 32/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 32/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   2.2s\n",
            "[CV 3/3; 32/64] START classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 32/64] END classify__alpha=0.0001, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.397 total time=   2.5s\n",
            "[CV 1/3; 33/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 33/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   1.6s\n",
            "[CV 2/3; 33/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 33/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.572 total time=   1.7s\n",
            "[CV 3/3; 33/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 33/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.627 total time=   1.7s\n",
            "[CV 1/3; 34/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 34/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.619 total time=   1.9s\n",
            "[CV 2/3; 34/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 34/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.602 total time=   1.9s\n",
            "[CV 3/3; 34/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 34/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.637 total time=   2.4s\n",
            "[CV 1/3; 35/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 35/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   2.7s\n",
            "[CV 2/3; 35/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 35/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.572 total time=   1.5s\n",
            "[CV 3/3; 35/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 35/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.627 total time=   1.5s\n",
            "[CV 1/3; 36/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 36/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.619 total time=   1.5s\n",
            "[CV 2/3; 36/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 36/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.602 total time=   1.5s\n",
            "[CV 3/3; 36/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 36/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.637 total time=   1.5s\n",
            "[CV 1/3; 37/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 37/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   1.6s\n",
            "[CV 2/3; 37/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 37/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.572 total time=   2.2s\n",
            "[CV 3/3; 37/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 37/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.627 total time=   2.7s\n",
            "[CV 1/3; 38/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 38/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.619 total time=   1.9s\n",
            "[CV 2/3; 38/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 38/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.602 total time=   1.9s\n",
            "[CV 3/3; 38/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 38/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.637 total time=   1.9s\n",
            "[CV 1/3; 39/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 39/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   1.6s\n",
            "[CV 2/3; 39/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 39/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.572 total time=   1.5s\n",
            "[CV 3/3; 39/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 39/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.627 total time=   1.8s\n",
            "[CV 1/3; 40/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 40/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.619 total time=   2.6s\n",
            "[CV 2/3; 40/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 40/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.602 total time=   1.5s\n",
            "[CV 3/3; 40/64] START classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 40/64] END classify__alpha=0.01, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.637 total time=   1.5s\n",
            "[CV 1/3; 41/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 41/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.6s\n",
            "[CV 2/3; 41/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 41/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.7s\n",
            "[CV 3/3; 41/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 41/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.7s\n",
            "[CV 1/3; 42/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 42/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   1.9s\n",
            "[CV 2/3; 42/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 42/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   3.0s\n",
            "[CV 3/3; 42/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 42/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.399 total time=   2.3s\n",
            "[CV 1/3; 43/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 43/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 2/3; 43/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 43/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 3/3; 43/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 43/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.5s\n",
            "[CV 1/3; 44/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 44/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   1.5s\n",
            "[CV 2/3; 44/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 44/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   1.5s\n",
            "[CV 3/3; 44/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 44/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.399 total time=   1.7s\n",
            "[CV 1/3; 45/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 45/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.7s\n",
            "[CV 2/3; 45/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 45/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.8s\n",
            "[CV 3/3; 45/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 45/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.7s\n",
            "[CV 1/3; 46/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 46/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   1.9s\n",
            "[CV 2/3; 46/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 46/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.445 total time=   1.9s\n",
            "[CV 3/3; 46/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 46/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.399 total time=   1.9s\n",
            "[CV 1/3; 47/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 47/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.9s\n",
            "[CV 2/3; 47/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 47/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.9s\n",
            "[CV 3/3; 47/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 47/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.5s\n",
            "[CV 1/3; 48/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 48/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   1.5s\n",
            "[CV 2/3; 48/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 48/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.445 total time=   1.5s\n",
            "[CV 3/3; 48/64] START classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 48/64] END classify__alpha=0.01, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.399 total time=   1.5s\n",
            "[CV 1/3; 49/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 49/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.602 total time=   1.6s\n",
            "[CV 2/3; 49/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 49/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.8s\n",
            "[CV 3/3; 49/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 49/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.631 total time=   2.7s\n",
            "[CV 1/3; 50/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 50/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.610 total time=   2.4s\n",
            "[CV 2/3; 50/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 50/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.602 total time=   1.9s\n",
            "[CV 3/3; 50/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 50/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.642 total time=   1.9s\n",
            "[CV 1/3; 51/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 51/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.602 total time=   1.5s\n",
            "[CV 2/3; 51/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 51/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.5s\n",
            "[CV 3/3; 51/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 51/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.631 total time=   1.5s\n",
            "[CV 1/3; 52/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 52/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.610 total time=   2.3s\n",
            "[CV 2/3; 52/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 52/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.602 total time=   2.1s\n",
            "[CV 3/3; 52/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 52/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.642 total time=   1.5s\n",
            "[CV 1/3; 53/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 53/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.602 total time=   1.6s\n",
            "[CV 2/3; 53/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 53/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.7s\n",
            "[CV 3/3; 53/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 53/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.631 total time=   1.7s\n",
            "[CV 1/3; 54/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 54/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.610 total time=   1.9s\n",
            "[CV 2/3; 54/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 54/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.602 total time=   2.4s\n",
            "[CV 3/3; 54/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 54/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.642 total time=   2.6s\n",
            "[CV 1/3; 55/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 55/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.602 total time=   1.6s\n",
            "[CV 2/3; 55/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 55/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.5s\n",
            "[CV 3/3; 55/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 55/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.631 total time=   1.5s\n",
            "[CV 1/3; 56/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 56/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.610 total time=   1.5s\n",
            "[CV 2/3; 56/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 56/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.602 total time=   1.5s\n",
            "[CV 3/3; 56/64] START classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 56/64] END classify__alpha=0.1, classify__binarize=0.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.642 total time=   1.5s\n",
            "[CV 1/3; 57/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 57/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.8s\n",
            "[CV 2/3; 57/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 57/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.8s\n",
            "[CV 3/3; 57/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 57/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.7s\n",
            "[CV 1/3; 58/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 58/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   1.9s\n",
            "[CV 2/3; 58/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 58/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   1.9s\n",
            "[CV 3/3; 58/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 58/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.408 total time=   1.9s\n",
            "[CV 1/3; 59/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 59/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.8s\n",
            "[CV 2/3; 59/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 59/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   2.7s\n",
            "[CV 3/3; 59/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 59/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.6s\n",
            "[CV 1/3; 60/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 60/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   1.5s\n",
            "[CV 2/3; 60/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 60/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   1.5s\n",
            "[CV 3/3; 60/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 60/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=True, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.408 total time=   1.5s\n",
            "[CV 1/3; 61/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 61/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.6s\n",
            "[CV 2/3; 61/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 61/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.7s\n",
            "[CV 3/3; 61/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 61/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   2.2s\n",
            "[CV 1/3; 62/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 1/3; 62/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   2.6s\n",
            "[CV 2/3; 62/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 2/3; 62/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.441 total time=   1.9s\n",
            "[CV 3/3; 62/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer()\n",
            "[CV 3/3; 62/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=True, vectorize=CountVectorizer();, score=0.408 total time=   1.9s\n",
            "[CV 1/3; 63/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 63/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.6s\n",
            "[CV 2/3; 63/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 63/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.248 total time=   1.5s\n",
            "[CV 3/3; 63/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 63/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.249 total time=   1.8s\n",
            "[CV 1/3; 64/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 1/3; 64/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   2.2s\n",
            "[CV 2/3; 64/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 2/3; 64/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.441 total time=   2.2s\n",
            "[CV 3/3; 64/64] START classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer()\n",
            "[CV 3/3; 64/64] END classify__alpha=0.1, classify__binarize=1.0, classify__fit_prior=False, to_dense__apply_dense=False, vectorize=CountVectorizer();, score=0.408 total time=   1.5s\n",
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5850 (± 0.0154)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0109)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5850 (± 0.0154)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0109)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5850 (± 0.0154)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0109)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5850 (± 0.0154)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0109)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 1e-06, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0189)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6157 (± 0.0138)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0189)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6157 (± 0.0138)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0189)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6157 (± 0.0138)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0189)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6157 (± 0.0138)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.0001, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4271 (± 0.0215)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5957 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0145)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5957 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0145)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5957 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0145)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5957 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0145)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4286 (± 0.0209)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4286 (± 0.0209)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4286 (± 0.0209)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.01, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4286 (± 0.0209)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5972 (± 0.0296)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6179 (± 0.0172)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5972 (± 0.0296)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6179 (± 0.0172)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5972 (± 0.0296)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6179 (± 0.0172)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5972 (± 0.0296)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 0.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6179 (± 0.0172)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4300 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': True, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4300 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4300 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.2486 (± 0.0003)\n",
            "\n",
            "Parameters: {'classify__alpha': 0.1, 'classify__binarize': 1.0, 'classify__fit_prior': False, 'to_dense__apply_dense': False, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4300 (± 0.0157)\n",
            "\n",
            "Best parameters: {'classify__alpha': 0.01, 'classify__binarize': 0.0, 'classify__fit_prior': True, 'to_dense__apply_dense': True, 'vectorize': CountVectorizer()}\n",
            "Best cross-validation accuracy: 0.6192986003253348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "5fLjiaNBmHnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define accuracy scorer using make_scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Define parameter grid for Decision Tree with vectorizer, max_depth, min_samples_split, min_samples_leaf, max_features, and dense transformation\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],  # Different vectorizers\n",
        "    'classify__max_depth': [None, 20],  # Maximum depth of the tree\n",
        "    'classify__min_samples_split': [2, 10],  # Minimum number of samples required to split an internal node\n",
        "    'classify__min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "    'classify__max_features': ['auto', 'sqrt', None],  # Number of features to consider for best split\n",
        "    'classify__criterion': ['gini', 'entropy']  # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "# Conditional step in the pipeline based on the 'apply_dense' parameter\n",
        "class ConditionalDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, apply_dense=True):  # Set apply_dense=True by default\n",
        "        self.apply_dense = apply_dense\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return DenseTransformer().transform(X) if self.apply_dense else X\n",
        "\n",
        "# Updated pipeline with conditional dense transformation set to apply_dense=True\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),  # Placeholder, updated by grid search\n",
        "    ('to_dense', ConditionalDenseTransformer(apply_dense=True)),  # Always apply dense transformation\n",
        "    ('classify', DecisionTreeClassifier())  # Decision Tree classifier\n",
        "])\n",
        "\n",
        "# Grid search setup with new parameter grid, using the custom accuracy scorer\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,  # 2-fold cross-validation for quick testing, adjust as needed\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(train_body, subreddit)\n",
        "\n",
        "# Display each combination's parameters and corresponding cross-validation results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and cross-validation score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model to predict on test_body\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Save the results\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions})\n",
        "results_df.to_csv(\"final_predictions_with_best_params_decision_tree.csv\", index=False)\n",
        "print(\"\\nPredictions saved to final_predictions_with_best_params_decision_tree.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYz72OfmKgS",
        "outputId": "6ee49c92-39aa-4040-e465-8f5cfbc81f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n",
            "[CV 1/2; 1/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 1/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 1/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 1/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 2/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 2/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.2s\n",
            "[CV 2/2; 2/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 2/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 1/2; 3/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 3/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.1s\n",
            "[CV 2/2; 3/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 3/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 4/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 4/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 4/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 4/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 5/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 5/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 5/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 5/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 6/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 6/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 6/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 6/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 7/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 7/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 7/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 7/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 8/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 8/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 8/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 8/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 9/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 9/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 9/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 9/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.0s\n",
            "[CV 1/2; 10/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 10/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 2/2; 10/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 10/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.4s\n",
            "[CV 1/2; 11/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 11/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 11/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 11/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 12/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 12/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 12/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 12/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 13/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 13/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.507 total time=   1.5s\n",
            "[CV 2/2; 13/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 13/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.451 total time=   1.5s\n",
            "[CV 1/2; 14/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 14/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.459 total time=   1.6s\n",
            "[CV 2/2; 14/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 14/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.514 total time=   1.6s\n",
            "[CV 1/2; 15/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 15/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.459 total time=   2.1s\n",
            "[CV 2/2; 15/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 15/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.493 total time=   2.4s\n",
            "[CV 1/2; 16/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 16/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.406 total time=   1.6s\n",
            "[CV 2/2; 16/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 16/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.407 total time=   1.6s\n",
            "[CV 1/2; 17/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 17/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.509 total time=   1.5s\n",
            "[CV 2/2; 17/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 17/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.493 total time=   1.5s\n",
            "[CV 1/2; 18/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 18/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.450 total time=   1.6s\n",
            "[CV 2/2; 18/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 18/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.447 total time=   1.6s\n",
            "[CV 1/2; 19/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 19/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.400 total time=   2.3s\n",
            "[CV 2/2; 19/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 19/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.469 total time=   2.4s\n",
            "[CV 1/2; 20/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 20/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.450 total time=   1.6s\n",
            "[CV 2/2; 20/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 20/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.459 total time=   1.6s\n",
            "[CV 1/2; 21/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 21/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.501 total time=   1.5s\n",
            "[CV 2/2; 21/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 21/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.423 total time=   1.5s\n",
            "[CV 1/2; 22/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 22/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.386 total time=   1.6s\n",
            "[CV 2/2; 22/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 22/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.349 total time=   1.6s\n",
            "[CV 1/2; 23/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 23/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.491 total time=   2.4s\n",
            "[CV 2/2; 23/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 23/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.504 total time=   2.2s\n",
            "[CV 1/2; 24/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 24/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.471 total time=   1.6s\n",
            "[CV 2/2; 24/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 24/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.363 total time=   1.6s\n",
            "[CV 1/2; 25/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 25/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.560 total time=   1.7s\n",
            "[CV 2/2; 25/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 25/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.556 total time=   1.7s\n",
            "[CV 1/2; 26/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 26/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.544 total time=   2.2s\n",
            "[CV 2/2; 26/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 26/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.567 total time=   3.4s\n",
            "[CV 1/2; 27/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 27/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.563 total time=   2.1s\n",
            "[CV 2/2; 27/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 27/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.563 total time=   1.7s\n",
            "[CV 1/2; 28/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 28/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.544 total time=   2.2s\n",
            "[CV 2/2; 28/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 28/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.546 total time=   2.2s\n",
            "[CV 1/2; 29/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 29/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.551 total time=   1.7s\n",
            "[CV 2/2; 29/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 29/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.520 total time=   2.1s\n",
            "[CV 1/2; 30/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 30/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.547 total time=   3.4s\n",
            "[CV 2/2; 30/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 30/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.547 total time=   2.2s\n",
            "[CV 1/2; 31/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 31/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.561 total time=   1.7s\n",
            "[CV 2/2; 31/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 31/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.7s\n",
            "[CV 1/2; 32/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 32/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.550 total time=   2.2s\n",
            "[CV 2/2; 32/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 32/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.589 total time=   2.5s\n",
            "[CV 1/2; 33/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 33/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   2.7s\n",
            "[CV 2/2; 33/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 33/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.560 total time=   1.7s\n",
            "[CV 1/2; 34/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 34/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.523 total time=   2.1s\n",
            "[CV 2/2; 34/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 34/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.530 total time=   2.1s\n",
            "[CV 1/2; 35/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 35/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.561 total time=   1.7s\n",
            "[CV 2/2; 35/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 35/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.547 total time=   1.6s\n",
            "[CV 1/2; 36/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 36/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.520 total time=   3.2s\n",
            "[CV 2/2; 36/144] START classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 36/144] END classify__criterion=gini, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.530 total time=   2.4s\n",
            "[CV 1/2; 37/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 37/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 37/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 37/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 38/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 38/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 38/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 38/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 39/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 39/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 39/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 39/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 40/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 40/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 40/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 40/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 41/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 41/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 41/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 41/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 42/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 42/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.1s\n",
            "[CV 2/2; 42/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 42/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 1/2; 43/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 43/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 2/2; 43/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 43/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 1/2; 44/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 44/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 44/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 44/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 45/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 45/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 45/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 45/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 1/2; 46/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 46/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 46/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 46/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 47/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 47/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 47/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 47/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 48/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 48/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 48/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 48/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 49/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 49/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.409 total time=   1.6s\n",
            "[CV 2/2; 49/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 49/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.416 total time=   2.6s\n",
            "[CV 1/2; 50/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 50/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.327 total time=   1.9s\n",
            "[CV 2/2; 50/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 50/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.387 total time=   1.6s\n",
            "[CV 1/2; 51/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 51/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.341 total time=   1.5s\n",
            "[CV 2/2; 51/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 51/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.400 total time=   1.5s\n",
            "[CV 1/2; 52/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 52/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.396 total time=   1.6s\n",
            "[CV 2/2; 52/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 52/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.366 total time=   1.6s\n",
            "[CV 1/2; 53/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 53/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.377 total time=   1.9s\n",
            "[CV 2/2; 53/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 53/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.430 total time=   2.6s\n",
            "[CV 1/2; 54/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 54/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.386 total time=   1.7s\n",
            "[CV 2/2; 54/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 54/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.330 total time=   1.6s\n",
            "[CV 1/2; 55/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 55/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.349 total time=   1.5s\n",
            "[CV 2/2; 55/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 55/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.329 total time=   1.5s\n",
            "[CV 1/2; 56/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 56/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.354 total time=   1.6s\n",
            "[CV 2/2; 56/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 56/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.360 total time=   1.6s\n",
            "[CV 1/2; 57/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 57/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.480 total time=   1.9s\n",
            "[CV 2/2; 57/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 57/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.357 total time=   2.6s\n",
            "[CV 1/2; 58/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 58/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.367 total time=   1.7s\n",
            "[CV 2/2; 58/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 58/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.376 total time=   1.6s\n",
            "[CV 1/2; 59/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 59/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.461 total time=   1.5s\n",
            "[CV 2/2; 59/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 59/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.416 total time=   1.5s\n",
            "[CV 1/2; 60/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 60/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.430 total time=   1.5s\n",
            "[CV 2/2; 60/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 60/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.390 total time=   1.6s\n",
            "[CV 1/2; 61/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 61/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.580 total time=   2.2s\n",
            "[CV 2/2; 61/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 61/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.557 total time=   2.6s\n",
            "[CV 1/2; 62/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 62/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.486 total time=   2.0s\n",
            "[CV 2/2; 62/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 62/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.516 total time=   2.0s\n",
            "[CV 1/2; 63/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 63/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.580 total time=   1.6s\n",
            "[CV 2/2; 63/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 63/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.566 total time=   1.6s\n",
            "[CV 1/2; 64/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 64/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.503 total time=   2.1s\n",
            "[CV 2/2; 64/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 64/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.547 total time=   3.3s\n",
            "[CV 1/2; 65/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 65/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.581 total time=   1.7s\n",
            "[CV 2/2; 65/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 65/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.563 total time=   1.6s\n",
            "[CV 1/2; 66/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 66/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.496 total time=   2.0s\n",
            "[CV 2/2; 66/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 66/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.543 total time=   2.1s\n",
            "[CV 1/2; 67/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 67/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.579 total time=   1.6s\n",
            "[CV 2/2; 67/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 67/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.570 total time=   2.0s\n",
            "[CV 1/2; 68/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 68/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.499 total time=   3.3s\n",
            "[CV 2/2; 68/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 68/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.544 total time=   2.0s\n",
            "[CV 1/2; 69/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 69/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.594 total time=   1.6s\n",
            "[CV 2/2; 69/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 69/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.570 total time=   1.6s\n",
            "[CV 1/2; 70/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 70/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.494 total time=   2.0s\n",
            "[CV 2/2; 70/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 70/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.517 total time=   2.1s\n",
            "[CV 1/2; 71/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 71/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.583 total time=   2.5s\n",
            "[CV 2/2; 71/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 71/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.567 total time=   2.4s\n",
            "[CV 1/2; 72/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 72/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.490 total time=   2.0s\n",
            "[CV 2/2; 72/144] START classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 72/144] END classify__criterion=gini, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.517 total time=   2.0s\n",
            "[CV 1/2; 73/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 73/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 73/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 73/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 74/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 74/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 74/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 74/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 75/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 75/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 75/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 75/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 76/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 76/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 76/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 76/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 1/2; 77/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 77/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 2/2; 77/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 77/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.1s\n",
            "[CV 1/2; 78/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 78/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 78/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 78/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 79/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 79/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 79/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 79/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 80/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 80/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 80/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 80/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 81/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 81/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 81/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 81/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 82/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 82/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 82/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 82/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 83/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 83/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 83/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 83/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 84/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 84/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   1.1s\n",
            "[CV 2/2; 84/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 84/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 1/2; 85/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 85/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.416 total time=   2.3s\n",
            "[CV 2/2; 85/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 85/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.500 total time=   1.5s\n",
            "[CV 1/2; 86/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 86/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.401 total time=   1.6s\n",
            "[CV 2/2; 86/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 86/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.431 total time=   1.6s\n",
            "[CV 1/2; 87/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 87/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.490 total time=   1.5s\n",
            "[CV 2/2; 87/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 87/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.499 total time=   1.5s\n",
            "[CV 1/2; 88/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 88/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.416 total time=   1.6s\n",
            "[CV 2/2; 88/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 88/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.494 total time=   2.5s\n",
            "[CV 1/2; 89/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 89/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.480 total time=   1.9s\n",
            "[CV 2/2; 89/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 89/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.463 total time=   1.5s\n",
            "[CV 1/2; 90/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 90/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.409 total time=   1.6s\n",
            "[CV 2/2; 90/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 90/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.501 total time=   1.6s\n",
            "[CV 1/2; 91/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 91/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.447 total time=   1.5s\n",
            "[CV 2/2; 91/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 91/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.423 total time=   1.5s\n",
            "[CV 1/2; 92/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 92/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.423 total time=   1.8s\n",
            "[CV 2/2; 92/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 92/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.416 total time=   2.7s\n",
            "[CV 1/2; 93/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 93/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.470 total time=   1.5s\n",
            "[CV 2/2; 93/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 93/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.431 total time=   1.5s\n",
            "[CV 1/2; 94/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 94/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.389 total time=   1.6s\n",
            "[CV 2/2; 94/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 94/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.407 total time=   1.6s\n",
            "[CV 1/2; 95/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 95/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.497 total time=   1.5s\n",
            "[CV 2/2; 95/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 95/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.436 total time=   1.5s\n",
            "[CV 1/2; 96/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 96/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.451 total time=   2.2s\n",
            "[CV 2/2; 96/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 96/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.436 total time=   2.6s\n",
            "[CV 1/2; 97/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 97/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.557 total time=   1.7s\n",
            "[CV 2/2; 97/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 97/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.530 total time=   1.7s\n",
            "[CV 1/2; 98/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 98/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.541 total time=   2.2s\n",
            "[CV 2/2; 98/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 98/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.540 total time=   2.3s\n",
            "[CV 1/2; 99/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 99/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.556 total time=   1.7s\n",
            "[CV 2/2; 99/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 99/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.549 total time=   2.7s\n",
            "[CV 1/2; 100/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 100/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.557 total time=   2.7s\n",
            "[CV 2/2; 100/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 100/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.557 total time=   2.2s\n",
            "[CV 1/2; 101/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 101/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.537 total time=   1.7s\n",
            "[CV 2/2; 101/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 101/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.543 total time=   1.7s\n",
            "[CV 1/2; 102/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 102/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.544 total time=   2.2s\n",
            "[CV 2/2; 102/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 102/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.557 total time=   3.2s\n",
            "[CV 1/2; 103/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 103/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.554 total time=   2.1s\n",
            "[CV 2/2; 103/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 103/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.533 total time=   1.7s\n",
            "[CV 1/2; 104/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 104/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.553 total time=   2.2s\n",
            "[CV 2/2; 104/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 104/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.564 total time=   2.2s\n",
            "[CV 1/2; 105/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 105/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.521 total time=   1.6s\n",
            "[CV 2/2; 105/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 105/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   2.0s\n",
            "[CV 1/2; 106/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 106/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.530 total time=   3.2s\n",
            "[CV 2/2; 106/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 106/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.560 total time=   2.1s\n",
            "[CV 1/2; 107/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 107/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.519 total time=   1.6s\n",
            "[CV 2/2; 107/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 107/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.560 total time=   1.7s\n",
            "[CV 1/2; 108/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 108/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.541 total time=   2.1s\n",
            "[CV 2/2; 108/144] START classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 108/144] END classify__criterion=entropy, classify__max_depth=None, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.567 total time=   2.3s\n",
            "[CV 1/2; 109/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 109/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 2/2; 109/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 109/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 1/2; 110/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 110/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.0s\n",
            "[CV 2/2; 110/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 110/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 111/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 111/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 111/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 111/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 112/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 112/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 112/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 112/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 113/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 113/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 2/2; 113/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 113/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 114/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 114/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 114/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 114/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 115/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 115/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 115/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 115/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 116/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 116/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 116/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 116/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   1.2s\n",
            "[CV 1/2; 117/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 117/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 2/2; 117/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 117/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 1/2; 118/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 118/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   1.0s\n",
            "[CV 2/2; 118/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 118/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 119/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 119/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 119/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 119/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 120/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 120/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 120/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 120/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=auto, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 121/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 121/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.373 total time=   1.5s\n",
            "[CV 2/2; 121/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 121/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.457 total time=   1.5s\n",
            "[CV 1/2; 122/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 122/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.356 total time=   1.6s\n",
            "[CV 2/2; 122/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 122/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.391 total time=   1.9s\n",
            "[CV 1/2; 123/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 123/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.437 total time=   2.6s\n",
            "[CV 2/2; 123/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 123/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.396 total time=   1.5s\n",
            "[CV 1/2; 124/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 124/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.311 total time=   1.6s\n",
            "[CV 2/2; 124/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 124/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.354 total time=   1.6s\n",
            "[CV 1/2; 125/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 125/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.397 total time=   1.5s\n",
            "[CV 2/2; 125/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 125/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.400 total time=   1.5s\n",
            "[CV 1/2; 126/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 126/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.351 total time=   1.6s\n",
            "[CV 2/2; 126/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 126/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.331 total time=   2.1s\n",
            "[CV 1/2; 127/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 127/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.404 total time=   2.4s\n",
            "[CV 2/2; 127/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 127/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.443 total time=   1.5s\n",
            "[CV 1/2; 128/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 128/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.329 total time=   1.6s\n",
            "[CV 2/2; 128/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 128/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.413 total time=   1.6s\n",
            "[CV 1/2; 129/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 129/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.416 total time=   1.5s\n",
            "[CV 2/2; 129/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 129/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.344 total time=   1.5s\n",
            "[CV 1/2; 130/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 130/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.359 total time=   1.6s\n",
            "[CV 2/2; 130/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 130/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.331 total time=   2.3s\n",
            "[CV 1/2; 131/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 131/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.404 total time=   2.2s\n",
            "[CV 2/2; 131/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 131/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.431 total time=   1.5s\n",
            "[CV 1/2; 132/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 132/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.354 total time=   1.6s\n",
            "[CV 2/2; 132/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 132/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=sqrt, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.371 total time=   1.6s\n",
            "[CV 1/2; 133/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 133/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   1.6s\n",
            "[CV 2/2; 133/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 133/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.554 total time=   1.6s\n",
            "[CV 1/2; 134/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 134/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.537 total time=   2.4s\n",
            "[CV 2/2; 134/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 134/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.551 total time=   3.3s\n",
            "[CV 1/2; 135/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 135/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.570 total time=   1.6s\n",
            "[CV 2/2; 135/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 135/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.554 total time=   1.6s\n",
            "[CV 1/2; 136/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 136/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.530 total time=   2.1s\n",
            "[CV 2/2; 136/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 136/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=1, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.556 total time=   2.1s\n",
            "[CV 1/2; 137/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 137/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=   1.6s\n",
            "[CV 2/2; 137/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 137/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.563 total time=   2.4s\n",
            "[CV 1/2; 138/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 138/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.527 total time=   2.8s\n",
            "[CV 2/2; 138/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 138/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.546 total time=   2.0s\n",
            "[CV 1/2; 139/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 139/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   1.6s\n",
            "[CV 2/2; 139/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 139/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.553 total time=   1.6s\n",
            "[CV 1/2; 140/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 140/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.539 total time=   2.0s\n",
            "[CV 2/2; 140/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 140/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=2, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.559 total time=   2.2s\n",
            "[CV 1/2; 141/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 141/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.574 total time=   2.8s\n",
            "[CV 2/2; 141/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 141/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   2.0s\n",
            "[CV 1/2; 142/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 1/2; 142/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.523 total time=   2.0s\n",
            "[CV 2/2; 142/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer()\n",
            "[CV 2/2; 142/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=2, vectorize=CountVectorizer();, score=0.554 total time=   2.0s\n",
            "[CV 1/2; 143/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 143/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   1.6s\n",
            "[CV 2/2; 143/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 143/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.559 total time=   1.6s\n",
            "[CV 1/2; 144/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 1/2; 144/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.526 total time=   2.4s\n",
            "[CV 2/2; 144/144] START classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer()\n",
            "[CV 2/2; 144/144] END classify__criterion=entropy, classify__max_depth=20, classify__max_features=None, classify__min_samples_leaf=4, classify__min_samples_split=10, vectorize=CountVectorizer();, score=0.559 total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "96 fits failed out of a total of 288.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "96 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.47928571 0.48642857 0.47571429 0.40642857 0.50071429 0.44857143\n",
            " 0.43428571 0.45428571 0.46214286 0.36714286 0.49785714 0.41714286\n",
            " 0.55785714 0.55571429 0.56285714 0.545      0.53571429 0.54714286\n",
            " 0.56       0.56928571 0.56642857 0.52642857 0.55428571 0.525\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.41214286 0.35714286 0.37071429 0.38071429 0.40357143 0.35785714\n",
            " 0.33857143 0.35714286 0.41857143 0.37142857 0.43857143 0.41\n",
            " 0.56857143 0.50071429 0.57285714 0.525      0.57214286 0.51928571\n",
            " 0.57428571 0.52142857 0.58214286 0.50571429 0.575      0.50357143\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.45785714 0.41642857 0.49428571 0.455      0.47142857 0.455\n",
            " 0.435      0.41928571 0.45071429 0.39785714 0.46642857 0.44357143\n",
            " 0.54357143 0.54071429 0.55214286 0.55714286 0.54       0.55071429\n",
            " 0.54357143 0.55857143 0.54285714 0.545      0.53928571 0.55428571\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.415      0.37357143 0.41642857 0.33285714 0.39857143 0.34142857\n",
            " 0.42357143 0.37071429 0.38       0.345      0.41785714 0.36285714\n",
            " 0.56357143 0.54428571 0.56214286 0.54285714 0.56714286 0.53642857\n",
            " 0.56285714 0.54857143 0.56928571 0.53857143 0.56571429 0.54214286]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4793 (± 0.0279)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4864 (± 0.0279)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4757 (± 0.0171)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4064 (± 0.0007)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5007 (± 0.0079)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4486 (± 0.0014)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4343 (± 0.0343)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4543 (± 0.0043)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4621 (± 0.0393)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3671 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4979 (± 0.0064)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4171 (± 0.0543)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5579 (± 0.0021)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5557 (± 0.0114)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5629 (± 0.0000)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5450 (± 0.0007)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5357 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5471 (± 0.0000)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5600 (± 0.0014)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5693 (± 0.0193)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5664 (± 0.0064)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5264 (± 0.0036)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5543 (± 0.0071)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5250 (± 0.0050)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4121 (± 0.0036)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3571 (± 0.0300)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.3707 (± 0.0293)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3807 (± 0.0150)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4036 (± 0.0264)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3579 (± 0.0279)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.3386 (± 0.0100)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3571 (± 0.0029)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4186 (± 0.0614)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3714 (± 0.0043)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4386 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4100 (± 0.0200)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5686 (± 0.0114)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5007 (± 0.0150)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5729 (± 0.0071)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5250 (± 0.0221)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5721 (± 0.0093)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5193 (± 0.0236)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5743 (± 0.0043)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5214 (± 0.0229)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5821 (± 0.0121)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5057 (± 0.0114)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5750 (± 0.0079)\n",
            "\n",
            "Parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5036 (± 0.0136)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4579 (± 0.0421)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4164 (± 0.0150)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4943 (± 0.0043)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4550 (± 0.0393)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4714 (± 0.0086)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4550 (± 0.0464)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4350 (± 0.0121)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4193 (± 0.0036)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4507 (± 0.0193)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3979 (± 0.0093)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4664 (± 0.0307)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4436 (± 0.0079)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5436 (± 0.0136)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5407 (± 0.0007)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5521 (± 0.0036)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5571 (± 0.0000)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5400 (± 0.0029)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5507 (± 0.0064)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5436 (± 0.0107)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5586 (± 0.0057)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5429 (± 0.0214)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5450 (± 0.0150)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5393 (± 0.0207)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': None, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5543 (± 0.0129)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'auto', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4150 (± 0.0421)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3736 (± 0.0179)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4164 (± 0.0207)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3329 (± 0.0214)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.3986 (± 0.0014)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3414 (± 0.0100)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4236 (± 0.0193)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3707 (± 0.0421)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.3800 (± 0.0357)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3450 (± 0.0136)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.4179 (± 0.0136)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': 'sqrt', 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3629 (± 0.0086)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5636 (± 0.0093)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5443 (± 0.0071)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5621 (± 0.0079)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5429 (± 0.0129)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5671 (± 0.0043)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5364 (± 0.0093)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5629 (± 0.0100)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5486 (± 0.0100)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5693 (± 0.0050)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5386 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5657 (± 0.0071)\n",
            "\n",
            "Parameters: {'classify__criterion': 'entropy', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 10, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5421 (± 0.0164)\n",
            "\n",
            "Best parameters: {'classify__criterion': 'gini', 'classify__max_depth': 20, 'classify__max_features': None, 'classify__min_samples_leaf': 4, 'classify__min_samples_split': 2, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Best cross-validation accuracy: 0.5821428571428571\n",
            "\n",
            "Predictions saved to final_predictions_with_best_params_decision_tree.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "0QJj9CvSp7th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define accuracy scorer using make_scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Define parameter grid for Logistic Regression with vectorizer, regularization (C), and solver\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],  # Different vectorizers\n",
        "    'classify__C': [0.01, 0.1, 1.0, 10],  # Regularization strength (inverse of alpha)\n",
        "    'classify__penalty': ['l2', 'none'],  # Regularization penalty\n",
        "    'classify__solver': ['lbfgs', 'saga'],  # Solvers compatible with L2 penalty and dense data\n",
        "}\n",
        "\n",
        "# Conditional step in the pipeline based on the 'apply_dense' parameter, set to False\n",
        "class ConditionalDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, apply_dense=False):\n",
        "        self.apply_dense = apply_dense\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return DenseTransformer().transform(X) if self.apply_dense else X\n",
        "\n",
        "# Updated pipeline with dense transformation set to False\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),  # Placeholder, updated by grid search\n",
        "    ('to_dense', ConditionalDenseTransformer(apply_dense=False)),  # Set dense transformation to False\n",
        "    ('classify', LogisticRegression(max_iter=1000))  # Logistic Regression classifier with increased iterations\n",
        "])\n",
        "\n",
        "# Grid search setup with the updated parameter grid and custom accuracy scorer\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,  # Adjust as needed\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10  # Higher verbosity for detailed output\n",
        ")\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(train_body, subreddit)\n",
        "\n",
        "# Display each combination's parameters and corresponding cross-validation results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and cross-validation score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model to predict on test_body\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Save the results\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions})\n",
        "results_df.to_csv(\"final_predictions_with_best_params_logistic_regression.csv\", index=False)\n",
        "print(\"\\nPredictions saved to final_predictions_with_best_params_logistic_regression.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2kIcgHyp-kW",
        "outputId": "789a24a4-6f8b-40f6-c799-b901e1294fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n",
            "[CV 1/2; 1/32] START classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 1/32] END classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=   1.5s\n",
            "[CV 2/2; 1/32] START classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 1/32] END classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.651 total time=   1.5s\n",
            "[CV 1/2; 2/32] START classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 2/32] END classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.596 total time=   1.7s\n",
            "[CV 2/2; 2/32] START classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 2/32] END classify__C=0.01, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.666 total time=   1.7s\n",
            "[CV 1/2; 3/32] START classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 3/32] END classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.566 total time=   2.4s\n",
            "[CV 2/2; 3/32] START classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 3/32] END classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.620 total time=   2.2s\n",
            "[CV 1/2; 4/32] START classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 4/32] END classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.613 total time=   1.5s\n",
            "[CV 2/2; 4/32] START classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 4/32] END classify__C=0.01, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.686 total time=   1.5s\n",
            "[CV 1/2; 5/32] START classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 5/32] END classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 2/2; 5/32] START classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 5/32] END classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 6/32] START classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 6/32] END classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 6/32] START classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 6/32] END classify__C=0.01, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 7/32] START classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 7/32] END classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 2/2; 7/32] START classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 7/32] END classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 8/32] START classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 8/32] END classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 8/32] START classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 8/32] END classify__C=0.01, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.7s\n",
            "[CV 1/2; 9/32] START classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 9/32] END classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=   2.4s\n",
            "[CV 2/2; 9/32] START classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 9/32] END classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.656 total time=   2.2s\n",
            "[CV 1/2; 10/32] START classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 10/32] END classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.644 total time=   1.8s\n",
            "[CV 2/2; 10/32] START classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 10/32] END classify__C=0.1, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.676 total time=   2.0s\n",
            "[CV 1/2; 11/32] START classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 11/32] END classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=   1.5s\n",
            "[CV 2/2; 11/32] START classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 11/32] END classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.659 total time=   1.5s\n",
            "[CV 1/2; 12/32] START classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 12/32] END classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.649 total time=   1.6s\n",
            "[CV 2/2; 12/32] START classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 12/32] END classify__C=0.1, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.686 total time=   2.2s\n",
            "[CV 1/2; 13/32] START classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 13/32] END classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 2/2; 13/32] START classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 13/32] END classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.3s\n",
            "[CV 1/2; 14/32] START classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 14/32] END classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 14/32] START classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 14/32] END classify__C=0.1, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 15/32] START classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 15/32] END classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 15/32] START classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 15/32] END classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 16/32] START classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 16/32] END classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.7s\n",
            "[CV 2/2; 16/32] START classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 16/32] END classify__C=0.1, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 17/32] START classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 17/32] END classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.611 total time=   1.6s\n",
            "[CV 2/2; 17/32] START classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 17/32] END classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.669 total time=   1.5s\n",
            "[CV 1/2; 18/32] START classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 18/32] END classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.644 total time=   2.1s\n",
            "[CV 2/2; 18/32] START classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 18/32] END classify__C=1.0, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.669 total time=   3.5s\n",
            "[CV 1/2; 19/32] START classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 19/32] END classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.614 total time=   2.0s\n",
            "[CV 2/2; 19/32] START classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 19/32] END classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.669 total time=   1.5s\n",
            "[CV 1/2; 20/32] START classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 20/32] END classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.651 total time=   1.9s\n",
            "[CV 2/2; 20/32] START classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 20/32] END classify__C=1.0, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.673 total time=   1.9s\n",
            "[CV 1/2; 21/32] START classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 21/32] END classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 21/32] START classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 21/32] END classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 22/32] START classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 22/32] END classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 22/32] START classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 22/32] END classify__C=1.0, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.7s\n",
            "[CV 1/2; 23/32] START classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 23/32] END classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 23/32] START classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 23/32] END classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   1.2s\n",
            "[CV 1/2; 24/32] START classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 24/32] END classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 2/2; 24/32] START classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 24/32] END classify__C=1.0, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   1.3s\n",
            "[CV 1/2; 25/32] START classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 25/32] END classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.630 total time=   1.8s\n",
            "[CV 2/2; 25/32] START classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 25/32] END classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.661 total time=   1.6s\n",
            "[CV 1/2; 26/32] START classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 26/32] END classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.656 total time=   1.8s\n",
            "[CV 2/2; 26/32] START classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 26/32] END classify__C=10, classify__penalty=l2, classify__solver=lbfgs, vectorize=CountVectorizer();, score=0.659 total time=   1.9s\n",
            "[CV 1/2; 27/32] START classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 27/32] END classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.626 total time=   1.5s\n",
            "[CV 2/2; 27/32] START classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 27/32] END classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.669 total time=   1.5s\n",
            "[CV 1/2; 28/32] START classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 28/32] END classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.657 total time=   3.9s\n",
            "[CV 2/2; 28/32] START classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 28/32] END classify__C=10, classify__penalty=l2, classify__solver=saga, vectorize=CountVectorizer();, score=0.667 total time=   2.4s\n",
            "[CV 1/2; 29/32] START classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 29/32] END classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 29/32] START classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 29/32] END classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.7s\n",
            "[CV 1/2; 30/32] START classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 1/2; 30/32] END classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 30/32] START classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer()\n",
            "[CV 2/2; 30/32] END classify__C=10, classify__penalty=none, classify__solver=lbfgs, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 1/2; 31/32] START classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 31/32] END classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 2/2; 31/32] START classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 31/32] END classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=nan total time=   0.8s\n",
            "[CV 1/2; 32/32] START classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 1/2; 32/32] END classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.8s\n",
            "[CV 2/2; 32/32] START classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer()\n",
            "[CV 2/2; 32/32] END classify__C=10, classify__penalty=none, classify__solver=saga, vectorize=CountVectorizer();, score=nan total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "32 fits failed out of a total of 64.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "32 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 473, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.61142857 0.63071429 0.59285714 0.64928571        nan        nan\n",
            "        nan        nan 0.62071429 0.66       0.62214286 0.66714286\n",
            "        nan        nan        nan        nan 0.64       0.65642857\n",
            " 0.64142857 0.66214286        nan        nan        nan        nan\n",
            " 0.64571429 0.65714286 0.64714286 0.66214286        nan        nan\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6114 (± 0.0400)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6307 (± 0.0350)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5929 (± 0.0271)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6493 (± 0.0364)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6207 (± 0.0350)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6600 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6221 (± 0.0364)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6671 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 0.1, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6400 (± 0.0286)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6564 (± 0.0121)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6414 (± 0.0271)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6621 (± 0.0107)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 1.0, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6457 (± 0.0157)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'l2', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6571 (± 0.0014)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6471 (± 0.0214)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6621 (± 0.0050)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'none', 'classify__solver': 'lbfgs', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__penalty': 'none', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: nan (± nan)\n",
            "\n",
            "Best parameters: {'classify__C': 0.1, 'classify__penalty': 'l2', 'classify__solver': 'saga', 'vectorize': CountVectorizer()}\n",
            "Best cross-validation accuracy: 0.6671428571428571\n",
            "\n",
            "Predictions saved to final_predictions_with_best_params_logistic_regression.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVD + SVM**"
      ],
      "metadata": {
        "id": "KPYsycUWq4XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define accuracy scorer using make_scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Define parameter grid for SVM with vectorizer, SVD components, C, kernel, and gamma\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],  # Different vectorizers\n",
        "    'svd__n_components': [200, 600],  # SVD components\n",
        "    'classify__C': [0.01, 1, 10],  # Regularization parameter\n",
        "    'classify__kernel': ['linear', 'sigmoid'],  # Different kernels\n",
        "    'classify__gamma': ['scale', 'auto']  # Kernel coefficient\n",
        "}\n",
        "\n",
        "# Conditional step in the pipeline based on the 'apply_dense' parameter, set to False\n",
        "class ConditionalDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, apply_dense=False):\n",
        "        self.apply_dense = apply_dense\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return DenseTransformer().transform(X) if self.apply_dense else X\n",
        "\n",
        "# Updated pipeline with dense transformation set to False and SVD step\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),  # Placeholder, updated by grid search\n",
        "    ('to_dense', ConditionalDenseTransformer(apply_dense=False)),  # Set dense transformation to False\n",
        "    ('svd', TruncatedSVD()),  # Placeholder for SVD, parameter updated by grid search\n",
        "    ('classify', SVC())  # SVM classifier\n",
        "])\n",
        "\n",
        "# Grid search setup with the updated parameter grid and custom accuracy scorer\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,  # Adjust as needed\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10  # Higher verbosity for detailed output\n",
        ")\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(train_body, subreddit)\n",
        "\n",
        "# Display each combination's parameters and corresponding cross-validation results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and cross-validation score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Use the best model to predict on test_body\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Save the results\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions})\n",
        "results_df.to_csv(\"final_predictions_with_best_params_svm.csv\", index=False)\n",
        "print(\"\\nPredictions saved to final_predictions_with_best_params_svm.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKrCokLirHBm",
        "outputId": "9b862231-bcdd-4135-e2f4-9acbfd1d6674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n",
            "[CV 1/2; 1/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 1/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.464 total time=   4.1s\n",
            "[CV 2/2; 1/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 1/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.569 total time=   1.9s\n",
            "[CV 1/2; 2/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 2/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.547 total time=   2.4s\n",
            "[CV 2/2; 2/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 2/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.573 total time=   2.5s\n",
            "[CV 1/2; 3/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 3/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.479 total time=   3.5s\n",
            "[CV 2/2; 3/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 3/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.577 total time=   4.9s\n",
            "[CV 1/2; 4/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 4/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.560 total time=   5.4s\n",
            "[CV 2/2; 4/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 4/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.613 total time=   9.4s\n",
            "[CV 1/2; 5/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 5/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.513 total time=   2.0s\n",
            "[CV 2/2; 5/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 5/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   2.0s\n",
            "[CV 1/2; 6/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 6/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.387 total time=   2.5s\n",
            "[CV 2/2; 6/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 6/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.383 total time=   2.9s\n",
            "[CV 1/2; 7/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 7/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.516 total time=   5.2s\n",
            "[CV 2/2; 7/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 7/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.577 total time=   3.1s\n",
            "[CV 1/2; 8/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 8/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.391 total time=   5.5s\n",
            "[CV 2/2; 8/48] START classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 8/48] END classify__C=0.01, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.363 total time=   8.4s\n",
            "[CV 1/2; 9/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 9/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.471 total time=   1.9s\n",
            "[CV 2/2; 9/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 9/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.566 total time=   1.9s\n",
            "[CV 1/2; 10/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 10/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.537 total time=   2.7s\n",
            "[CV 2/2; 10/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 10/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.574 total time=   5.1s\n",
            "[CV 1/2; 11/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 11/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.481 total time=   3.0s\n",
            "[CV 2/2; 11/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 11/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   3.0s\n",
            "[CV 1/2; 12/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 12/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.567 total time=   9.5s\n",
            "[CV 2/2; 12/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 12/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.611 total time=   5.4s\n",
            "[CV 1/2; 13/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 13/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.473 total time=   2.0s\n",
            "[CV 2/2; 13/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 13/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=   2.5s\n",
            "[CV 1/2; 14/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 14/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.403 total time=   4.5s\n",
            "[CV 2/2; 14/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 14/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.327 total time=   2.5s\n",
            "[CV 1/2; 15/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 15/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.480 total time=   3.1s\n",
            "[CV 2/2; 15/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 15/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.577 total time=   3.0s\n",
            "[CV 1/2; 16/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 16/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.386 total time=   7.6s\n",
            "[CV 2/2; 16/48] START classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 16/48] END classify__C=0.01, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.307 total time=   5.4s\n",
            "[CV 1/2; 17/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 17/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.583 total time=   2.0s\n",
            "[CV 2/2; 17/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 17/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.651 total time=   3.3s\n",
            "[CV 1/2; 18/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 18/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.606 total time=   2.5s\n",
            "[CV 2/2; 18/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 18/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.610 total time=   2.4s\n",
            "[CV 1/2; 19/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 19/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.606 total time=   3.0s\n",
            "[CV 2/2; 19/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 19/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=   3.5s\n",
            "[CV 1/2; 20/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 20/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.593 total time=   6.7s\n",
            "[CV 2/2; 20/48] START classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 20/48] END classify__C=1, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.646 total time=   5.4s\n",
            "[CV 1/2; 21/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 21/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.610 total time=   2.6s\n",
            "[CV 2/2; 21/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 21/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.647 total time=   3.1s\n",
            "[CV 1/2; 22/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 22/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.564 total time=   4.4s\n",
            "[CV 2/2; 22/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 22/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.623 total time=   2.5s\n",
            "[CV 1/2; 23/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 23/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.604 total time=   3.6s\n",
            "[CV 2/2; 23/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 23/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.651 total time=   4.1s\n",
            "[CV 1/2; 24/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 24/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.600 total time=   5.4s\n",
            "[CV 2/2; 24/48] START classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 24/48] END classify__C=1, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.644 total time=   8.4s\n",
            "[CV 1/2; 25/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 25/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.590 total time=   1.9s\n",
            "[CV 2/2; 25/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 25/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.647 total time=   2.0s\n",
            "[CV 1/2; 26/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 26/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.600 total time=   2.4s\n",
            "[CV 2/2; 26/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 26/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.614 total time=   2.6s\n",
            "[CV 1/2; 27/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 27/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.609 total time=   5.6s\n",
            "[CV 2/2; 27/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 27/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.651 total time=   3.0s\n",
            "[CV 1/2; 28/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 28/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.596 total time=   5.4s\n",
            "[CV 2/2; 28/48] START classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 28/48] END classify__C=1, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.630 total time=   7.4s\n",
            "[CV 1/2; 29/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 29/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.467 total time=   1.9s\n",
            "[CV 2/2; 29/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 29/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.563 total time=   1.9s\n",
            "[CV 1/2; 30/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 30/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.487 total time=   2.5s\n",
            "[CV 2/2; 30/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 30/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.481 total time=   4.6s\n",
            "[CV 1/2; 31/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 31/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.479 total time=   3.3s\n",
            "[CV 2/2; 31/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 31/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.574 total time=   3.0s\n",
            "[CV 1/2; 32/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 32/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.499 total time=   6.1s\n",
            "[CV 2/2; 32/48] START classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 32/48] END classify__C=1, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.429 total time=   6.3s\n",
            "[CV 1/2; 33/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 33/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   1.9s\n",
            "[CV 2/2; 33/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 33/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.620 total time=   1.9s\n",
            "[CV 1/2; 34/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 34/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.571 total time=   2.6s\n",
            "[CV 2/2; 34/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 34/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.589 total time=   5.3s\n",
            "[CV 1/2; 35/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 35/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.613 total time=   3.0s\n",
            "[CV 2/2; 35/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 35/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.649 total time=   3.0s\n",
            "[CV 1/2; 36/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 36/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.547 total time=   9.4s\n",
            "[CV 2/2; 36/48] START classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 36/48] END classify__C=10, classify__gamma=scale, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.629 total time=   5.3s\n",
            "[CV 1/2; 37/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 37/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.609 total time=   1.9s\n",
            "[CV 2/2; 37/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 37/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.626 total time=   2.4s\n",
            "[CV 1/2; 38/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 38/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.569 total time=   4.2s\n",
            "[CV 2/2; 38/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 38/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.591 total time=   2.4s\n",
            "[CV 1/2; 39/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 39/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   3.0s\n",
            "[CV 2/2; 39/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 39/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.626 total time=   3.0s\n",
            "[CV 1/2; 40/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 40/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.560 total time=   7.2s\n",
            "[CV 2/2; 40/48] START classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 40/48] END classify__C=10, classify__gamma=scale, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.607 total time=   5.4s\n",
            "[CV 1/2; 41/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 41/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.619 total time=   1.9s\n",
            "[CV 2/2; 41/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 41/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.637 total time=   3.5s\n",
            "[CV 1/2; 42/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 42/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.559 total time=   2.7s\n",
            "[CV 2/2; 42/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 42/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=200, vectorize=CountVectorizer();, score=0.583 total time=   2.5s\n",
            "[CV 1/2; 43/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 43/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.617 total time=   3.0s\n",
            "[CV 2/2; 43/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 43/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.643 total time=   3.3s\n",
            "[CV 1/2; 44/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 44/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.546 total time=   7.0s\n",
            "[CV 2/2; 44/48] START classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 44/48] END classify__C=10, classify__gamma=auto, classify__kernel=linear, svd__n_components=600, vectorize=CountVectorizer();, score=0.631 total time=   5.3s\n",
            "[CV 1/2; 45/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 45/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.467 total time=   2.4s\n",
            "[CV 2/2; 45/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 45/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   4.2s\n",
            "[CV 1/2; 46/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 1/2; 46/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.587 total time=   2.4s\n",
            "[CV 2/2; 46/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer()\n",
            "[CV 2/2; 46/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=200, vectorize=CountVectorizer();, score=0.629 total time=   2.5s\n",
            "[CV 1/2; 47/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/2; 47/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.479 total time=   3.0s\n",
            "[CV 2/2; 47/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/2; 47/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.573 total time=   5.5s\n",
            "[CV 1/2; 48/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 1/2; 48/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.583 total time=   5.4s\n",
            "[CV 2/2; 48/48] START classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer()\n",
            "[CV 2/2; 48/48] END classify__C=10, classify__gamma=auto, classify__kernel=sigmoid, svd__n_components=600, vectorize=CountVectorizer();, score=0.641 total time=  11.7s\n",
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5164 (± 0.0521)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5600 (± 0.0129)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5279 (± 0.0493)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5864 (± 0.0264)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5386 (± 0.0257)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3850 (± 0.0021)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5464 (± 0.0307)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3771 (± 0.0143)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5186 (± 0.0471)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5557 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5271 (± 0.0457)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5893 (± 0.0221)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5221 (± 0.0493)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3650 (± 0.0379)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5286 (± 0.0486)\n",
            "\n",
            "Parameters: {'classify__C': 0.01, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.3464 (± 0.0393)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6171 (± 0.0343)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6079 (± 0.0021)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6279 (± 0.0221)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0264)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6286 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5936 (± 0.0293)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6279 (± 0.0236)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6221 (± 0.0221)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6186 (± 0.0286)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6071 (± 0.0071)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6300 (± 0.0214)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0171)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5150 (± 0.0479)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4843 (± 0.0029)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5264 (± 0.0479)\n",
            "\n",
            "Parameters: {'classify__C': 1, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.4636 (± 0.0350)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6207 (± 0.0007)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5800 (± 0.0086)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6307 (± 0.0179)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0407)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6171 (± 0.0086)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5800 (± 0.0114)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6071 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5836 (± 0.0236)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6279 (± 0.0093)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5707 (± 0.0121)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6300 (± 0.0129)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5886 (± 0.0429)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5157 (± 0.0486)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6079 (± 0.0207)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5257 (± 0.0471)\n",
            "\n",
            "Parameters: {'classify__C': 10, 'classify__gamma': 'auto', 'classify__kernel': 'sigmoid', 'svd__n_components': 600, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6121 (± 0.0293)\n",
            "\n",
            "Best parameters: {'classify__C': 10, 'classify__gamma': 'scale', 'classify__kernel': 'linear', 'svd__n_components': 600, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Best cross-validation accuracy: 0.6307142857142858\n",
            "\n",
            "Predictions saved to final_predictions_with_best_params_svm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "MrfyZpm-vaiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define accuracy scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Define parameter grid for RandomForestClassifier with different vectorizers and hyperparameters\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],  # Different vectorizers\n",
        "    'classify__n_estimators': [100, 200],  # Number of trees in the forest\n",
        "    'classify__max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
        "    'classify__min_samples_split': [2, 5],  # Minimum samples required to split an internal node\n",
        "    'classify__min_samples_leaf': [1, 2]  # Minimum samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Define a pipeline with RandomForestClassifier and no SVD\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),  # Placeholder; updated by grid search\n",
        "    ('classify', RandomForestClassifier(random_state=42))  # Random Forest classifier\n",
        "])\n",
        "\n",
        "# Initialize GridSearchCV with the pipeline and parameter grid\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Fit grid search to the data\n",
        "grid_search.fit(train_body, subreddit)\n",
        "\n",
        "# Display grid search results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and cross-validation score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Predict on test_body with the best model\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Save predictions\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions})\n",
        "results_df.to_csv(\"final_predictions_with_best_params_rf.csv\", index=False)\n",
        "print(\"\\nPredictions saved to final_predictions_with_best_params_rf.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULTTJU1bvc5R",
        "outputId": "e5c4fd4c-2b3f-48e1-8617-619bc6a60cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "[CV 1/3; 1/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 1/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.653 total time=   2.3s\n",
            "[CV 2/3; 1/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 1/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.597 total time=   2.3s\n",
            "[CV 3/3; 1/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 1/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.648 total time=   2.3s\n",
            "[CV 1/3; 2/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 2/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.636 total time=   4.0s\n",
            "[CV 2/3; 2/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 2/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.565 total time=   2.6s\n",
            "[CV 3/3; 2/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 2/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.676 total time=   2.7s\n",
            "[CV 1/3; 3/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 3/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   3.0s\n",
            "[CV 2/3; 3/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 3/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.602 total time=   3.8s\n",
            "[CV 3/3; 3/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 3/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   3.7s\n",
            "[CV 1/3; 4/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 4/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.621 total time=   3.8s\n",
            "[CV 2/3; 4/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 4/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.574 total time=   3.7s\n",
            "[CV 3/3; 4/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 4/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.687 total time=   5.2s\n",
            "[CV 1/3; 5/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 5/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   2.2s\n",
            "[CV 2/3; 5/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 5/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.591 total time=   2.2s\n",
            "[CV 3/3; 5/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 5/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.646 total time=   2.2s\n",
            "[CV 1/3; 6/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 6/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.627 total time=   2.4s\n",
            "[CV 2/3; 6/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 6/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.593 total time=   3.8s\n",
            "[CV 3/3; 6/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 6/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.667 total time=   2.8s\n",
            "[CV 1/3; 7/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 7/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.675 total time=   2.9s\n",
            "[CV 2/3; 7/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 7/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.600 total time=   2.8s\n",
            "[CV 3/3; 7/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 7/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.672 total time=   3.3s\n",
            "[CV 1/3; 8/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 8/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.630 total time=   4.6s\n",
            "[CV 2/3; 8/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 8/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.591 total time=   3.3s\n",
            "[CV 3/3; 8/48] START classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 8/48] END classify__max_depth=None, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.680 total time=   3.3s\n",
            "[CV 1/3; 9/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 9/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.677 total time=   2.1s\n",
            "[CV 2/3; 9/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 9/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.604 total time=   3.3s\n",
            "[CV 3/3; 9/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 9/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.676 total time=   2.3s\n",
            "[CV 1/3; 10/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 10/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.634 total time=   2.1s\n",
            "[CV 2/3; 10/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 10/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.608 total time=   2.0s\n",
            "[CV 3/3; 10/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 10/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.663 total time=   2.0s\n",
            "[CV 1/3; 11/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 11/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.672 total time=   2.7s\n",
            "[CV 2/3; 11/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 11/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.615 total time=   3.9s\n",
            "[CV 3/3; 11/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 11/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.676 total time=   2.5s\n",
            "[CV 1/3; 12/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 12/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.636 total time=   2.5s\n",
            "[CV 2/3; 12/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 12/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.604 total time=   2.5s\n",
            "[CV 3/3; 12/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 12/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.680 total time=   2.8s\n",
            "[CV 1/3; 13/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 13/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.679 total time=   3.3s\n",
            "[CV 2/3; 13/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 13/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.612 total time=   2.0s\n",
            "[CV 3/3; 13/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 13/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.670 total time=   2.0s\n",
            "[CV 1/3; 14/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 14/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.653 total time=   2.0s\n",
            "[CV 2/3; 14/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 14/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.612 total time=   2.0s\n",
            "[CV 3/3; 14/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 14/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.657 total time=   2.2s\n",
            "[CV 1/3; 15/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 15/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.679 total time=   4.0s\n",
            "[CV 2/3; 15/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 15/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.615 total time=   2.5s\n",
            "[CV 3/3; 15/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 15/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.680 total time=   2.5s\n",
            "[CV 1/3; 16/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 16/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.640 total time=   2.5s\n",
            "[CV 2/3; 16/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 16/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.606 total time=   2.5s\n",
            "[CV 3/3; 16/48] START classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 16/48] END classify__max_depth=None, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.678 total time=   4.0s\n",
            "[CV 1/3; 17/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 17/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.625 total time=   1.8s\n",
            "[CV 2/3; 17/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 17/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.578 total time=   1.8s\n",
            "[CV 3/3; 17/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 17/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.620 total time=   1.8s\n",
            "[CV 1/3; 18/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 18/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.602 total time=   1.8s\n",
            "[CV 2/3; 18/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 18/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.587 total time=   1.8s\n",
            "[CV 3/3; 18/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 18/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.633 total time=   2.4s\n",
            "[CV 1/3; 19/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 19/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.617 total time=   3.3s\n",
            "[CV 2/3; 19/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 19/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.615 total time=   2.0s\n",
            "[CV 3/3; 19/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 19/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=   2.0s\n",
            "[CV 1/3; 20/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 20/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.621 total time=   2.0s\n",
            "[CV 2/3; 20/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 20/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.608 total time=   2.0s\n",
            "[CV 3/3; 20/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 20/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.657 total time=   2.4s\n",
            "[CV 1/3; 21/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 21/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   2.8s\n",
            "[CV 2/3; 21/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 21/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.570 total time=   1.8s\n",
            "[CV 3/3; 21/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 21/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.633 total time=   1.8s\n",
            "[CV 1/3; 22/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 22/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.602 total time=   1.7s\n",
            "[CV 2/3; 22/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 22/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.593 total time=   1.8s\n",
            "[CV 3/3; 22/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 22/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.650 total time=   2.2s\n",
            "[CV 1/3; 23/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 23/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.619 total time=   6.2s\n",
            "[CV 2/3; 23/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 23/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.597 total time=   4.0s\n",
            "[CV 3/3; 23/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 23/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.659 total time=   3.4s\n",
            "[CV 1/3; 24/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 24/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.623 total time=   5.0s\n",
            "[CV 2/3; 24/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 24/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.617 total time=   3.9s\n",
            "[CV 3/3; 24/48] START classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 24/48] END classify__max_depth=10, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.672 total time=   2.0s\n",
            "[CV 1/3; 25/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 25/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.632 total time=   1.7s\n",
            "[CV 2/3; 25/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 25/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.585 total time=   2.0s\n",
            "[CV 3/3; 25/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 25/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.655 total time=   7.0s\n",
            "[CV 1/3; 26/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 26/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.608 total time=   3.8s\n",
            "[CV 2/3; 26/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 26/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.602 total time=   4.9s\n",
            "[CV 3/3; 26/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 26/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.642 total time=   2.4s\n",
            "[CV 1/3; 27/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 27/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.619 total time=   2.8s\n",
            "[CV 2/3; 27/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 27/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   2.0s\n",
            "[CV 3/3; 27/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 27/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   2.0s\n",
            "[CV 1/3; 28/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 28/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.632 total time=   1.9s\n",
            "[CV 2/3; 28/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 28/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.593 total time=   2.0s\n",
            "[CV 3/3; 28/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 28/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.674 total time=   2.4s\n",
            "[CV 1/3; 29/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 29/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.640 total time=   3.0s\n",
            "[CV 2/3; 29/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 29/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.570 total time=   1.8s\n",
            "[CV 3/3; 29/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 29/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.648 total time=   1.8s\n",
            "[CV 1/3; 30/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 30/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.612 total time=   1.7s\n",
            "[CV 2/3; 30/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 30/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.602 total time=   1.7s\n",
            "[CV 3/3; 30/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 30/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.650 total time=   1.7s\n",
            "[CV 1/3; 31/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 31/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   2.5s\n",
            "[CV 2/3; 31/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 31/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.585 total time=   3.0s\n",
            "[CV 3/3; 31/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 31/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.646 total time=   2.0s\n",
            "[CV 1/3; 32/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 32/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.630 total time=   1.9s\n",
            "[CV 2/3; 32/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 32/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.597 total time=   2.0s\n",
            "[CV 3/3; 32/48] START classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 32/48] END classify__max_depth=10, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.674 total time=   1.9s\n",
            "[CV 1/3; 33/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 33/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.623 total time=   2.1s\n",
            "[CV 2/3; 33/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 33/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.595 total time=   3.2s\n",
            "[CV 3/3; 33/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 33/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.648 total time=   1.9s\n",
            "[CV 1/3; 34/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 34/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.595 total time=   1.9s\n",
            "[CV 2/3; 34/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 34/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.602 total time=   1.9s\n",
            "[CV 3/3; 34/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 34/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.648 total time=   1.9s\n",
            "[CV 1/3; 35/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 35/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.638 total time=   2.3s\n",
            "[CV 2/3; 35/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 35/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.600 total time=   3.8s\n",
            "[CV 3/3; 35/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 35/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.642 total time=   2.5s\n",
            "[CV 1/3; 36/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 36/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.619 total time=   2.4s\n",
            "[CV 2/3; 36/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 36/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.600 total time=   2.4s\n",
            "[CV 3/3; 36/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 36/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.667 total time=   2.4s\n",
            "[CV 1/3; 37/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 37/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.636 total time=   2.5s\n",
            "[CV 2/3; 37/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 37/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   2.7s\n",
            "[CV 3/3; 37/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 37/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=   1.9s\n",
            "[CV 1/3; 38/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 38/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.619 total time=   1.9s\n",
            "[CV 2/3; 38/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 38/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.585 total time=   1.9s\n",
            "[CV 3/3; 38/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 38/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.655 total time=   1.9s\n",
            "[CV 1/3; 39/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 39/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.649 total time=   2.6s\n",
            "[CV 2/3; 39/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 39/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.608 total time=   3.5s\n",
            "[CV 3/3; 39/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 39/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.652 total time=   2.2s\n",
            "[CV 1/3; 40/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 40/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.617 total time=   2.3s\n",
            "[CV 2/3; 40/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 40/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.587 total time=   2.3s\n",
            "[CV 3/3; 40/48] START classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 40/48] END classify__max_depth=20, classify__min_samples_leaf=1, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.663 total time=   2.3s\n",
            "[CV 1/3; 41/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 41/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.651 total time=   2.6s\n",
            "[CV 2/3; 41/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 41/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.606 total time=   2.5s\n",
            "[CV 3/3; 41/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 41/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.648 total time=   1.8s\n",
            "[CV 1/3; 42/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 42/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.630 total time=   1.8s\n",
            "[CV 2/3; 42/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 42/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.587 total time=   1.8s\n",
            "[CV 3/3; 42/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 42/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.652 total time=   1.8s\n",
            "[CV 1/3; 43/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 43/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   2.8s\n",
            "[CV 2/3; 43/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 43/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.608 total time=   3.2s\n",
            "[CV 3/3; 43/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 43/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.661 total time=   2.1s\n",
            "[CV 1/3; 44/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 44/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.642 total time=   2.1s\n",
            "[CV 2/3; 44/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 44/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.595 total time=   2.1s\n",
            "[CV 3/3; 44/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 44/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=2, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.674 total time=   2.1s\n",
            "[CV 1/3; 45/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 45/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.647 total time=   2.5s\n",
            "[CV 2/3; 45/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 45/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.608 total time=   2.9s\n",
            "[CV 3/3; 45/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 45/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.646 total time=   1.8s\n",
            "[CV 1/3; 46/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 1/3; 46/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.615 total time=   1.8s\n",
            "[CV 2/3; 46/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 2/3; 46/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.595 total time=   1.8s\n",
            "[CV 3/3; 46/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer()\n",
            "[CV 3/3; 46/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=100, vectorize=CountVectorizer();, score=0.659 total time=   1.8s\n",
            "[CV 1/3; 47/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/3; 47/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.660 total time=   2.2s\n",
            "[CV 2/3; 47/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/3; 47/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.617 total time=   3.3s\n",
            "[CV 3/3; 47/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/3; 47/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.667 total time=   2.4s\n",
            "[CV 1/3; 48/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 1/3; 48/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.627 total time=   2.1s\n",
            "[CV 2/3; 48/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 2/3; 48/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.593 total time=   2.1s\n",
            "[CV 3/3; 48/48] START classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer()\n",
            "[CV 3/3; 48/48] END classify__max_depth=20, classify__min_samples_leaf=2, classify__min_samples_split=5, classify__n_estimators=200, vectorize=CountVectorizer();, score=0.676 total time=   2.1s\n",
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6329 (± 0.0251)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6258 (± 0.0457)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6386 (± 0.0261)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6272 (± 0.0463)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6314 (± 0.0290)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6293 (± 0.0303)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6486 (± 0.0347)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6336 (± 0.0365)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6522 (± 0.0342)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6350 (± 0.0225)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6543 (± 0.0281)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6400 (± 0.0313)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6536 (± 0.0294)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6407 (± 0.0201)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6579 (± 0.0306)\n",
            "\n",
            "Parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6415 (± 0.0295)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6079 (± 0.0211)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6072 (± 0.0193)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6272 (± 0.0163)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6286 (± 0.0205)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6079 (± 0.0275)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6150 (± 0.0251)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6250 (± 0.0254)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6372 (± 0.0245)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6236 (± 0.0291)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6172 (± 0.0175)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6215 (± 0.0277)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6329 (± 0.0329)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0353)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6214 (± 0.0208)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6172 (± 0.0252)\n",
            "\n",
            "Parameters: {'classify__max_depth': 10, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6336 (± 0.0313)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6222 (± 0.0216)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6150 (± 0.0235)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6264 (± 0.0191)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6286 (± 0.0285)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6250 (± 0.0262)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6193 (± 0.0285)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6364 (± 0.0201)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 1, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6222 (± 0.0314)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6350 (± 0.0205)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6229 (± 0.0272)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6422 (± 0.0241)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 2, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6372 (± 0.0323)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6336 (± 0.0180)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 100, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6229 (± 0.0266)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6479 (± 0.0223)\n",
            "\n",
            "Parameters: {'classify__max_depth': 20, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6322 (± 0.0340)\n",
            "\n",
            "Best parameters: {'classify__max_depth': None, 'classify__min_samples_leaf': 2, 'classify__min_samples_split': 5, 'classify__n_estimators': 200, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Best cross-validation accuracy: 0.6578731316993074\n",
            "\n",
            "Predictions saved to final_predictions_with_best_params_rf.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost**"
      ],
      "metadata": {
        "id": "RL9Izw-dwpQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define accuracy scorer\n",
        "accuracy_scorer = make_scorer(accuracy_score)\n",
        "\n",
        "# Encode labels for XGBoost only\n",
        "label_encoder = LabelEncoder()\n",
        "subreddit_encoded = label_encoder.fit_transform(subreddit)  # Use these labels for training\n",
        "\n",
        "# Define parameter grid for XGBoost with vectorizer options\n",
        "param_grid = {\n",
        "    'vectorize': [vectorizers['TF-IDF'], vectorizers['Count Vectorizer']],\n",
        "    'classify__learning_rate': [0.001, 0.01, 0.1],         # Learning rate\n",
        "    'classify__n_estimators': [100, 200],                # Number of boosting rounds\n",
        "    'classify__max_depth': [3, 5],                       # Max depth of the trees\n",
        "    'classify__subsample': [0.7, 1.0]                    # Subsample ratio of the training instance\n",
        "}\n",
        "\n",
        "# Updated pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('lemmatize', LemmatizerTransformer()),\n",
        "    ('vectorize', vectorizers['TF-IDF']),                # Placeholder; updated by grid search\n",
        "    ('classify', XGBClassifier(eval_metric='mlogloss'))\n",
        "])\n",
        "\n",
        "# Grid search setup with XGBoost\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                    # 5-fold cross-validation\n",
        "    scoring=accuracy_scorer,\n",
        "    refit=True,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Fit grid search to the encoded labels\n",
        "grid_search.fit(train_body, subreddit_encoded)\n",
        "\n",
        "# Display grid search results\n",
        "print(\"Grid search results:\\n\")\n",
        "for i, params in enumerate(grid_search.cv_results_['params']):\n",
        "    mean_score = grid_search.cv_results_['mean_test_score'][i]\n",
        "    std_score = grid_search.cv_results_['std_test_score'][i]\n",
        "    print(f\"Parameters: {params}\")\n",
        "    print(f\"Mean cross-validation accuracy: {mean_score:.4f} (± {std_score:.4f})\\n\")\n",
        "\n",
        "# Display the best parameters and score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Predict on test_body with the best model\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "test_predictions = best_pipeline.predict(test_body)\n",
        "\n",
        "# Convert numeric predictions back to original labels for final output\n",
        "test_predictions_labels = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "# Save predictions\n",
        "results_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions_labels})\n",
        "results_df.to_csv(\"final_predictions_with_best_params_xgboost.csv\", index=False)\n",
        "print(\"\\nPredictions saved to final_predictions_with_best_params_xgboost.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ra9MnWwsQ0",
        "outputId": "c46f226d-1386-4bcc-9e3e-6524009d96e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "[CV 1/5; 1/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 1/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=   6.4s\n",
            "[CV 2/5; 1/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 1/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   5.0s\n",
            "[CV 3/5; 1/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 1/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.525 total time=   4.3s\n",
            "[CV 4/5; 1/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 1/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.539 total time=   5.9s\n",
            "[CV 5/5; 1/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 1/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   4.4s\n",
            "[CV 1/5; 2/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 2/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.571 total time=   3.3s\n",
            "[CV 2/5; 2/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 2/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.543 total time=   3.8s\n",
            "[CV 3/5; 2/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 2/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.554 total time=   4.6s\n",
            "[CV 4/5; 2/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 2/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.539 total time=   3.2s\n",
            "[CV 5/5; 2/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 2/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.625 total time=   3.2s\n",
            "[CV 1/5; 3/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 3/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.554 total time=   6.5s\n",
            "[CV 2/5; 3/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 3/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.532 total time=   4.4s\n",
            "[CV 3/5; 3/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 3/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.521 total time=   4.5s\n",
            "[CV 4/5; 3/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 3/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.518 total time=   6.5s\n",
            "[CV 5/5; 3/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 3/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=   4.5s\n",
            "[CV 1/5; 4/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 4/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.550 total time=   3.3s\n",
            "[CV 2/5; 4/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 4/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.518 total time=   5.0s\n",
            "[CV 3/5; 4/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 4/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.529 total time=   3.3s\n",
            "[CV 4/5; 4/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 4/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.496 total time=   3.3s\n",
            "[CV 5/5; 4/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 4/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.514 total time=   3.4s\n",
            "[CV 1/5; 5/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 5/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=   9.0s\n",
            "[CV 2/5; 5/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 5/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   8.4s\n",
            "[CV 3/5; 5/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 5/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.532 total time=   7.3s\n",
            "[CV 4/5; 5/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 5/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.532 total time=   9.1s\n",
            "[CV 5/5; 5/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 5/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   7.2s\n",
            "[CV 1/5; 6/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 6/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.579 total time=   7.0s\n",
            "[CV 2/5; 6/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 6/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.543 total time=   4.9s\n",
            "[CV 3/5; 6/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 6/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.554 total time=   5.8s\n",
            "[CV 4/5; 6/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 6/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.539 total time=   5.8s\n",
            "[CV 5/5; 6/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 6/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.625 total time=   4.9s\n",
            "[CV 1/5; 7/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 7/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.557 total time=   9.5s\n",
            "[CV 2/5; 7/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 7/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.539 total time=   8.0s\n",
            "[CV 3/5; 7/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 7/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.521 total time=   8.3s\n",
            "[CV 4/5; 7/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 7/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.525 total time=   9.5s\n",
            "[CV 5/5; 7/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 7/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   7.7s\n",
            "[CV 1/5; 8/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 8/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.554 total time=   7.1s\n",
            "[CV 2/5; 8/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 8/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.529 total time=   5.1s\n",
            "[CV 3/5; 8/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 8/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.529 total time=   7.0s\n",
            "[CV 4/5; 8/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 8/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.518 total time=   5.2s\n",
            "[CV 5/5; 8/48] START classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 8/48] END classify__learning_rate=0.001, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.579 total time=   5.2s\n",
            "[CV 1/5; 9/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 9/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.611 total time=   8.5s\n",
            "[CV 2/5; 9/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 9/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.568 total time=   7.1s\n",
            "[CV 3/5; 9/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 9/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=   7.4s\n",
            "[CV 4/5; 9/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 9/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   8.4s\n",
            "[CV 5/5; 9/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 9/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.625 total time=   6.8s\n",
            "[CV 1/5; 10/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 10/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.586 total time=   6.5s\n",
            "[CV 2/5; 10/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 10/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.564 total time=   5.1s\n",
            "[CV 3/5; 10/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 10/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.589 total time=   5.1s\n",
            "[CV 4/5; 10/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 10/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.564 total time=   6.8s\n",
            "[CV 5/5; 10/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 10/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.632 total time=   5.0s\n",
            "[CV 1/5; 11/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 11/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=   9.2s\n",
            "[CV 2/5; 11/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 11/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.536 total time=   7.1s\n",
            "[CV 3/5; 11/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 11/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=   8.8s\n",
            "[CV 4/5; 11/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 11/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.550 total time=   8.2s\n",
            "[CV 5/5; 11/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 11/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   8.1s\n",
            "[CV 1/5; 12/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 12/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.550 total time=   6.5s\n",
            "[CV 2/5; 12/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 12/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.518 total time=   5.8s\n",
            "[CV 3/5; 12/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 12/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.539 total time=   5.3s\n",
            "[CV 4/5; 12/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 12/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.529 total time=   7.4s\n",
            "[CV 5/5; 12/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 12/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.564 total time=   5.1s\n",
            "[CV 1/5; 13/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 13/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.611 total time=  14.0s\n",
            "[CV 2/5; 13/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 13/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.582 total time=  13.3s\n",
            "[CV 3/5; 13/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 13/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.579 total time=  13.6s\n",
            "[CV 4/5; 13/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 13/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=  14.0s\n",
            "[CV 5/5; 13/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 13/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=  13.8s\n",
            "[CV 1/5; 14/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 14/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.593 total time=  10.9s\n",
            "[CV 2/5; 14/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 14/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.586 total time=   8.1s\n",
            "[CV 3/5; 14/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 14/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.589 total time=  10.6s\n",
            "[CV 4/5; 14/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 14/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.571 total time=  10.6s\n",
            "[CV 5/5; 14/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 14/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.646 total time=   8.4s\n",
            "[CV 1/5; 15/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 15/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=  15.2s\n",
            "[CV 2/5; 15/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 15/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.571 total time=  14.6s\n",
            "[CV 3/5; 15/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 15/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.596 total time=  14.5s\n",
            "[CV 4/5; 15/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 15/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.546 total time=  15.7s\n",
            "[CV 5/5; 15/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 15/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=  14.9s\n",
            "[CV 1/5; 16/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 16/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.550 total time=   9.3s\n",
            "[CV 2/5; 16/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 16/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.525 total time=  10.5s\n",
            "[CV 3/5; 16/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 16/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.543 total time=  11.3s\n",
            "[CV 4/5; 16/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 16/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.536 total time=  10.4s\n",
            "[CV 5/5; 16/48] START classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 16/48] END classify__learning_rate=0.001, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.582 total time=   8.9s\n",
            "[CV 1/5; 17/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 17/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.593 total time=   6.2s\n",
            "[CV 2/5; 17/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 17/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=   4.4s\n",
            "[CV 3/5; 17/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 17/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.575 total time=   4.3s\n",
            "[CV 4/5; 17/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 17/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.539 total time=   6.3s\n",
            "[CV 5/5; 17/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 17/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   4.4s\n",
            "[CV 1/5; 18/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 18/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.586 total time=   3.3s\n",
            "[CV 2/5; 18/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 18/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.582 total time=   4.3s\n",
            "[CV 3/5; 18/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 18/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.579 total time=   4.1s\n",
            "[CV 4/5; 18/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 18/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.564 total time=   3.3s\n",
            "[CV 5/5; 18/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 18/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.629 total time=   3.2s\n",
            "[CV 1/5; 19/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 19/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=   6.5s\n",
            "[CV 2/5; 19/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 19/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   4.5s\n",
            "[CV 3/5; 19/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 19/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.561 total time=   4.6s\n",
            "[CV 4/5; 19/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 19/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.543 total time=   6.4s\n",
            "[CV 5/5; 19/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 19/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.614 total time=   4.6s\n",
            "[CV 1/5; 20/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 20/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.568 total time=   3.4s\n",
            "[CV 2/5; 20/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 20/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.568 total time=   5.1s\n",
            "[CV 3/5; 20/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 20/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.571 total time=   3.3s\n",
            "[CV 4/5; 20/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 20/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.554 total time=   3.3s\n",
            "[CV 5/5; 20/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 20/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.607 total time=   4.0s\n",
            "[CV 1/5; 21/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 21/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.607 total time=   8.4s\n",
            "[CV 2/5; 21/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 21/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.582 total time=   8.7s\n",
            "[CV 3/5; 21/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 21/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   7.0s\n",
            "[CV 4/5; 21/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 21/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.561 total time=   9.0s\n",
            "[CV 5/5; 21/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 21/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.632 total time=   7.2s\n",
            "[CV 1/5; 22/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 22/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.596 total time=   7.0s\n",
            "[CV 2/5; 22/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 22/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.579 total time=   4.9s\n",
            "[CV 3/5; 22/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 22/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.607 total time=   6.4s\n",
            "[CV 4/5; 22/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 22/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.596 total time=   5.4s\n",
            "[CV 5/5; 22/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 22/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.646 total time=   5.0s\n",
            "[CV 1/5; 23/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 23/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.604 total time=   9.5s\n",
            "[CV 2/5; 23/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 23/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.561 total time=   9.0s\n",
            "[CV 3/5; 23/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 23/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=   7.6s\n",
            "[CV 4/5; 23/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 23/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=   9.4s\n",
            "[CV 5/5; 23/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 23/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.636 total time=   8.0s\n",
            "[CV 1/5; 24/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 24/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.589 total time=   6.8s\n",
            "[CV 2/5; 24/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 24/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.575 total time=   5.2s\n",
            "[CV 3/5; 24/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 24/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.607 total time=   7.3s\n",
            "[CV 4/5; 24/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 24/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.582 total time=   5.1s\n",
            "[CV 5/5; 24/48] START classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 24/48] END classify__learning_rate=0.01, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.621 total time=   6.1s\n",
            "[CV 1/5; 25/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 25/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.611 total time=   7.6s\n",
            "[CV 2/5; 25/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 25/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=   8.2s\n",
            "[CV 3/5; 25/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 25/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.614 total time=   6.6s\n",
            "[CV 4/5; 25/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 25/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.579 total time=   8.5s\n",
            "[CV 5/5; 25/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 25/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=   6.9s\n",
            "[CV 1/5; 26/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 26/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.607 total time=   6.9s\n",
            "[CV 2/5; 26/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 26/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.611 total time=   4.9s\n",
            "[CV 3/5; 26/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 26/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.614 total time=   6.1s\n",
            "[CV 4/5; 26/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 26/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.582 total time=   5.9s\n",
            "[CV 5/5; 26/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 26/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.650 total time=   5.0s\n",
            "[CV 1/5; 27/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 27/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.600 total time=   9.0s\n",
            "[CV 2/5; 27/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 27/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.579 total time=   7.8s\n",
            "[CV 3/5; 27/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 27/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=   7.7s\n",
            "[CV 4/5; 27/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 27/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.550 total time=   9.1s\n",
            "[CV 5/5; 27/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 27/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.643 total time=   7.2s\n",
            "[CV 1/5; 28/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 28/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.604 total time=   7.3s\n",
            "[CV 2/5; 28/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 28/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.589 total time=   5.2s\n",
            "[CV 3/5; 28/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 28/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.593 total time=   7.0s\n",
            "[CV 4/5; 28/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 28/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.571 total time=   5.5s\n",
            "[CV 5/5; 28/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 28/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.636 total time=   5.3s\n",
            "[CV 1/5; 29/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 29/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.614 total time=  13.8s\n",
            "[CV 2/5; 29/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 29/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.600 total time=  13.5s\n",
            "[CV 3/5; 29/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 29/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.632 total time=  13.7s\n",
            "[CV 4/5; 29/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 29/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.589 total time=  13.9s\n",
            "[CV 5/5; 29/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 29/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=  13.9s\n",
            "[CV 1/5; 30/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 30/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.625 total time=  11.1s\n",
            "[CV 2/5; 30/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 30/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.621 total time=  10.3s\n",
            "[CV 3/5; 30/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 30/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.611 total time=   8.9s\n",
            "[CV 4/5; 30/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 30/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.611 total time=  10.8s\n",
            "[CV 5/5; 30/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 30/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.675 total time=  10.8s\n",
            "[CV 1/5; 31/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 31/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.625 total time=  14.6s\n",
            "[CV 2/5; 31/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 31/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.586 total time=  14.0s\n",
            "[CV 3/5; 31/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 31/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=  14.5s\n",
            "[CV 4/5; 31/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 31/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.564 total time=  15.1s\n",
            "[CV 5/5; 31/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 31/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.664 total time=  14.7s\n",
            "[CV 1/5; 32/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 32/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.625 total time=  11.4s\n",
            "[CV 2/5; 32/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 32/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.596 total time=   8.8s\n",
            "[CV 3/5; 32/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 32/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.621 total time=  11.3s\n",
            "[CV 4/5; 32/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 32/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.593 total time=  11.2s\n",
            "[CV 5/5; 32/48] START classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 32/48] END classify__learning_rate=0.01, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.654 total time=  10.7s\n",
            "[CV 1/5; 33/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 33/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=   4.1s\n",
            "[CV 2/5; 33/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 33/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.654 total time=   4.0s\n",
            "[CV 3/5; 33/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 33/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.625 total time=   5.7s\n",
            "[CV 4/5; 33/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 33/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.607 total time=   4.3s\n",
            "[CV 5/5; 33/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 33/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.668 total time=   4.1s\n",
            "[CV 1/5; 34/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 34/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.632 total time=   4.2s\n",
            "[CV 2/5; 34/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 34/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.646 total time=   3.8s\n",
            "[CV 3/5; 34/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 34/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.639 total time=   3.2s\n",
            "[CV 4/5; 34/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 34/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.664 total time=   3.2s\n",
            "[CV 5/5; 34/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 34/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.696 total time=   4.8s\n",
            "[CV 1/5; 35/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 35/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.654 total time=   4.4s\n",
            "[CV 2/5; 35/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 35/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.625 total time=   4.1s\n",
            "[CV 3/5; 35/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 35/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.646 total time=   6.0s\n",
            "[CV 4/5; 35/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 35/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=   4.2s\n",
            "[CV 5/5; 35/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 35/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.671 total time=   4.3s\n",
            "[CV 1/5; 36/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 36/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.625 total time=   4.8s\n",
            "[CV 2/5; 36/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 36/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.632 total time=   3.4s\n",
            "[CV 3/5; 36/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 36/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.639 total time=   3.3s\n",
            "[CV 4/5; 36/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 36/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.632 total time=   3.2s\n",
            "[CV 5/5; 36/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 36/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.689 total time=   5.1s\n",
            "[CV 1/5; 37/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 37/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=   6.5s\n",
            "[CV 2/5; 37/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 37/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.636 total time=   8.1s\n",
            "[CV 3/5; 37/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 37/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.604 total time=   6.3s\n",
            "[CV 4/5; 37/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 37/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.611 total time=   8.2s\n",
            "[CV 5/5; 37/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 37/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.664 total time=   6.5s\n",
            "[CV 1/5; 38/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 38/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.636 total time=   6.4s\n",
            "[CV 2/5; 38/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 38/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.629 total time=   5.0s\n",
            "[CV 3/5; 38/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 38/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.632 total time=   4.9s\n",
            "[CV 4/5; 38/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 38/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.675 total time=   6.8s\n",
            "[CV 5/5; 38/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 38/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.704 total time=   4.8s\n",
            "[CV 1/5; 39/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 39/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.643 total time=   8.5s\n",
            "[CV 2/5; 39/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 39/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.657 total time=   6.5s\n",
            "[CV 3/5; 39/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 39/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.639 total time=   8.4s\n",
            "[CV 4/5; 39/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 39/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=   6.7s\n",
            "[CV 5/5; 39/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 39/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.707 total time=   8.7s\n",
            "[CV 1/5; 40/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 40/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.682 total time=   4.7s\n",
            "[CV 2/5; 40/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 40/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.650 total time=   6.1s\n",
            "[CV 3/5; 40/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 40/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.654 total time=   5.1s\n",
            "[CV 4/5; 40/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 40/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.664 total time=   4.8s\n",
            "[CV 5/5; 40/48] START classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 40/48] END classify__learning_rate=0.1, classify__max_depth=3, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.707 total time=   6.7s\n",
            "[CV 1/5; 41/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 41/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.629 total time=   6.5s\n",
            "[CV 2/5; 41/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 41/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.643 total time=   8.0s\n",
            "[CV 3/5; 41/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 41/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=   6.4s\n",
            "[CV 4/5; 41/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 41/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=   8.3s\n",
            "[CV 5/5; 41/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 41/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.686 total time=   6.5s\n",
            "[CV 1/5; 42/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 42/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.657 total time=   7.2s\n",
            "[CV 2/5; 42/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 42/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.654 total time=   5.0s\n",
            "[CV 3/5; 42/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 42/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.639 total time=   6.3s\n",
            "[CV 4/5; 42/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 42/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.657 total time=   6.0s\n",
            "[CV 5/5; 42/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 42/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.714 total time=   5.1s\n",
            "[CV 1/5; 43/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 43/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.643 total time=   8.5s\n",
            "[CV 2/5; 43/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 43/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=   6.4s\n",
            "[CV 3/5; 43/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 43/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=   8.0s\n",
            "[CV 4/5; 43/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 43/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.607 total time=   6.9s\n",
            "[CV 5/5; 43/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 43/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.714 total time=   7.9s\n",
            "[CV 1/5; 44/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 44/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.657 total time=   5.1s\n",
            "[CV 2/5; 44/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 44/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.636 total time=   6.6s\n",
            "[CV 3/5; 44/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 44/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.646 total time=   5.0s\n",
            "[CV 4/5; 44/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 44/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.632 total time=   6.9s\n",
            "[CV 5/5; 44/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 44/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=100, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.696 total time=   5.0s\n",
            "[CV 1/5; 45/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 45/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.646 total time=  12.0s\n",
            "[CV 2/5; 45/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 45/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.639 total time=  11.7s\n",
            "[CV 3/5; 45/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 45/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.607 total time=  10.4s\n",
            "[CV 4/5; 45/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 45/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.604 total time=  11.8s\n",
            "[CV 5/5; 45/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 45/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.679 total time=  12.2s\n",
            "[CV 1/5; 46/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 1/5; 46/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.657 total time=  10.4s\n",
            "[CV 2/5; 46/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 2/5; 46/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.625 total time=   8.0s\n",
            "[CV 3/5; 46/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 3/5; 46/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.643 total time=  10.0s\n",
            "[CV 4/5; 46/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 4/5; 46/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.664 total time=  10.1s\n",
            "[CV 5/5; 46/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer()\n",
            "[CV 5/5; 46/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=0.7, vectorize=CountVectorizer();, score=0.700 total time=   8.1s\n",
            "[CV 1/5; 47/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 1/5; 47/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.654 total time=  12.4s\n",
            "[CV 2/5; 47/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 2/5; 47/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.650 total time=  12.0s\n",
            "[CV 3/5; 47/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 3/5; 47/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.621 total time=  12.2s\n",
            "[CV 4/5; 47/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 4/5; 47/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.618 total time=  12.4s\n",
            "[CV 5/5; 47/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')\n",
            "[CV 5/5; 47/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}');, score=0.707 total time=  12.2s\n",
            "[CV 1/5; 48/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 1/5; 48/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.661 total time=   8.0s\n",
            "[CV 2/5; 48/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 2/5; 48/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.657 total time=   9.6s\n",
            "[CV 3/5; 48/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 3/5; 48/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.632 total time=   8.5s\n",
            "[CV 4/5; 48/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 4/5; 48/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.664 total time=   8.9s\n",
            "[CV 5/5; 48/48] START classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer()\n",
            "[CV 5/5; 48/48] END classify__learning_rate=0.1, classify__max_depth=5, classify__n_estimators=200, classify__subsample=1.0, vectorize=CountVectorizer();, score=0.700 total time=   9.9s\n",
            "Grid search results:\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5579 (± 0.0230)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5664 (± 0.0313)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5421 (± 0.0251)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5214 (± 0.0176)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5579 (± 0.0225)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5679 (± 0.0317)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5464 (± 0.0248)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5414 (± 0.0220)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5886 (± 0.0246)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5871 (± 0.0248)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5714 (± 0.0292)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5400 (± 0.0162)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5943 (± 0.0217)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5971 (± 0.0257)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5836 (± 0.0275)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.001, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5471 (± 0.0194)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5807 (± 0.0268)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5879 (± 0.0216)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5736 (± 0.0245)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5736 (± 0.0179)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5943 (± 0.0241)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6050 (± 0.0227)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5907 (± 0.0275)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5950 (± 0.0170)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6036 (± 0.0186)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6129 (± 0.0217)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.5979 (± 0.0319)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.5986 (± 0.0213)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6171 (± 0.0218)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6286 (± 0.0239)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6114 (± 0.0344)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.01, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6179 (± 0.0220)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6407 (± 0.0217)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6557 (± 0.0230)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6450 (± 0.0170)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6436 (± 0.0233)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6286 (± 0.0213)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6550 (± 0.0295)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6529 (± 0.0299)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6714 (± 0.0211)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6393 (± 0.0247)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6643 (± 0.0259)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6464 (± 0.0374)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 100, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6536 (± 0.0231)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6350 (± 0.0276)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 0.7, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6579 (± 0.0250)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': TfidfVectorizer(min_df=3, strip_accents='unicode', sublinear_tf=True,\n",
            "                token_pattern='\\\\w{1,}')}\n",
            "Mean cross-validation accuracy: 0.6500 (± 0.0320)\n",
            "\n",
            "Parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 5, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Mean cross-validation accuracy: 0.6629 (± 0.0217)\n",
            "\n",
            "Best parameters: {'classify__learning_rate': 0.1, 'classify__max_depth': 3, 'classify__n_estimators': 200, 'classify__subsample': 1.0, 'vectorize': CountVectorizer()}\n",
            "Best cross-validation accuracy: 0.6714285714285715\n",
            "\n",
            "Predictions saved to final_predictions_with_best_params_xgboost.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning: Majority Vote"
      ],
      "metadata": {
        "id": "MczMcFmguZ5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The voting process works on a per-instance basis for each test instance, combining predictions from each model in the ensemble to decide on the final prediction. Here’s how the process works step-by-step:\n",
        "\n",
        "\n",
        "1.   Individual Model Predictions: For each test instance, each model in the ensemble makes its own prediction independently.\n",
        "2.   Collect Predictions for Each Instance: After each model predicts for the test instance, the ensemble gathers the predictions from all models.\n",
        "3. Voting Mechanism:\n",
        "\n",
        "\n",
        "  *   In hard voting, the most common prediction (the “majority vote”) is chosen as the final prediction for that instance.\n",
        "  *   For soft voting (only applicable if models can output probabilities), each model assigns probabilities to each class, and the ensemble sums these probabilities across models. The class with the highest combined probability is chosen as the final prediction.\n",
        "\n",
        "4. Repeat for Each Test Instance: This voting process happens independently for each test instance, so the predictions are made one by one for each instance, resulting in a final array of predictions.\n",
        "\n",
        "So, to summarize, voting happens individually for each instance by aggregating predictions from all models and selecting the final class based on the chosen voting strategy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4lN2O2c9RKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference between hard and soft voting lies in how the final prediction is made in an ensemble of classifiers:\n",
        "\n",
        "1. Hard Voting:\n",
        "*   In hard voting, each base classifier makes a prediction (a \"vote\") for each sample, and the ensemble's final prediction is the label that receives the majority of the votes.\n",
        "\n",
        "*   It’s essentially a majority rule: if most classifiers predict one label, that becomes the final prediction.\n",
        "\n",
        "*   Hard voting works well when individual classifiers perform well and agree often.\n",
        "\n",
        "\\\\\n",
        "\n",
        "2. Soft Voting:\n",
        "*   In soft voting, each classifier provides a probability estimate for each class label rather than a fixed prediction.\n",
        "*   The ensemble takes the average of these probabilities for each class and predicts the class with the highest average probability.\n",
        "*   Soft voting generally performs better when the base classifiers are well-calibrated (meaning their probability estimates reflect their confidence) because it considers the confidence of each prediction rather than treating all predictions equally.\n",
        "*   This approach requires classifiers that can produce probability estimates (e.g., predict_proba method), such as Logistic Regression, Random Forest, and SVM when probability=True is set.\n",
        "\n",
        "In general, soft voting can outperform hard voting because it incorporates the degree of confidence each model has in its prediction, making it less sensitive to any individual model’s mistakes."
      ],
      "metadata": {
        "id": "ILLXSXxwCKPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize label encoder for XGBoost\n",
        "label_encoder = LabelEncoder()\n",
        "subreddit_encoded = label_encoder.fit_transform(subreddit)\n",
        "\n",
        "# Define vectorizers\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
        "    token_pattern=r'\\w{1,}', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
        "    lowercase=True\n",
        ")\n",
        "count_vectorizer = CountVectorizer(lowercase=True)\n",
        "\n",
        "# Set best parameters for each model (excluding vectorizer)\n",
        "best_params = {\n",
        "    'BernoulliNB': {'alpha': 0.01, 'binarize': 0.0, 'fit_prior': True},\n",
        "    'DecisionTree': {'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'gini'},\n",
        "    'LogisticRegression': {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'},\n",
        "    'SVM': {'C': 10.0, 'kernel': 'linear', 'gamma': 'scale', 'probability': True},  # with SVD\n",
        "    'RandomForest': {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 5, 'min_samples_leaf': 2},\n",
        "    'XGBoost': {'learning_rate': 0.1, 'n_estimators': 400, 'max_depth': 3, 'subsample': 1.0}\n",
        "}\n",
        "\n",
        "# Define the function to evaluate the pipeline\n",
        "def evaluate_pipeline(voting_type='hard'):\n",
        "    # Base models with best parameters, specify vectorizer within each model pipeline\n",
        "    models = [\n",
        "        ('bernoulli_nb', Pipeline([\n",
        "            ('vectorize', count_vectorizer),  # Use Count Vectorizer for BernoulliNB\n",
        "            ('classify', BernoulliNB(**best_params['BernoulliNB']))\n",
        "        ])),\n",
        "        ('decision_tree', Pipeline([\n",
        "            ('vectorize', tfidf_vectorizer),  # Use TF-IDF for Decision Tree\n",
        "            ('classify', DecisionTreeClassifier(**best_params['DecisionTree']))\n",
        "        ])),\n",
        "        ('logistic_reg', Pipeline([\n",
        "            ('vectorize', count_vectorizer),  # Use Count Vectorizer for Logistic Regression\n",
        "            ('classify', LogisticRegression(**best_params['LogisticRegression'], max_iter=2000))\n",
        "        ])),\n",
        "        ('svm', Pipeline([\n",
        "            ('vectorize', tfidf_vectorizer),  # Use TF-IDF for SVM\n",
        "            ('svd', TruncatedSVD(n_components=1000)),  # SVD for dimensionality reduction with SVM\n",
        "            ('classify', SVC(**best_params['SVM']))\n",
        "        ])),\n",
        "        ('random_forest', Pipeline([\n",
        "            ('vectorize', tfidf_vectorizer),  # Use TF-IDF for Random Forest\n",
        "            ('classify', RandomForestClassifier(**best_params['RandomForest']))\n",
        "        ])),\n",
        "        ('xgboost', Pipeline([\n",
        "            ('vectorize', count_vectorizer),  # Use Count Vectorizer for XGBoost\n",
        "            ('classify', XGBClassifier(**best_params['XGBoost'], eval_metric='mlogloss'))\n",
        "        ]))\n",
        "    ]\n",
        "\n",
        "    # Voting classifier with specified voting type\n",
        "    voting_clf = VotingClassifier(estimators=models, voting=voting_type)\n",
        "\n",
        "    # Set up K-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_accuracies = []\n",
        "    fold_times = []\n",
        "\n",
        "    print(f\"Evaluating with {voting_type} voting across 5 folds...\\n\")\n",
        "    overall_start = time.time()\n",
        "\n",
        "    # Loop through each fold\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_body), 1):\n",
        "        fold_start = time.time()\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_val = train_body.iloc[train_index], train_body.iloc[val_index]\n",
        "        y_train, y_val = subreddit_encoded[train_index], subreddit_encoded[val_index]\n",
        "\n",
        "        # Fit the voting classifier\n",
        "        voting_clf.fit(X_train, y_train)\n",
        "\n",
        "        # Predict and evaluate\n",
        "        val_predictions = voting_clf.predict(X_val)\n",
        "        fold_accuracy = accuracy_score(y_val, val_predictions)\n",
        "\n",
        "        # Record time and accuracy for the fold\n",
        "        fold_end = time.time()\n",
        "        fold_runtime = fold_end - fold_start\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "        fold_times.append(fold_runtime)\n",
        "\n",
        "        print(f\"Fold {fold}: Accuracy = {fold_accuracy:.4f}, Runtime = {fold_runtime:.2f} seconds\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    avg_runtime = np.mean(fold_times)\n",
        "    overall_runtime = time.time() - overall_start\n",
        "\n",
        "    print(f\"\\nAverage Accuracy ({voting_type} voting): {avg_accuracy:.4f}\")\n",
        "    print(f\"Average Fold Runtime: {avg_runtime:.2f} seconds\")\n",
        "    print(f\"Total Runtime: {overall_runtime:.2f} seconds\\n\")\n",
        "\n",
        "# Run the function for both voting types\n",
        "evaluate_pipeline(voting_type='hard')\n",
        "evaluate_pipeline(voting_type='soft')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g30I1bhK_sxn",
        "outputId": "2aa89766-3db1-4b5f-a15f-c0243b21651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating with hard voting across 5 folds...\n",
            "\n",
            "Fold 1: Accuracy = 0.6821, Runtime = 21.08 seconds\n",
            "Fold 2: Accuracy = 0.6536, Runtime = 26.25 seconds\n",
            "Fold 3: Accuracy = 0.6643, Runtime = 24.24 seconds\n",
            "Fold 4: Accuracy = 0.7036, Runtime = 20.28 seconds\n",
            "Fold 5: Accuracy = 0.6821, Runtime = 24.02 seconds\n",
            "\n",
            "Average Accuracy (hard voting): 0.6771\n",
            "Average Fold Runtime: 23.18 seconds\n",
            "Total Runtime: 115.89 seconds\n",
            "\n",
            "Evaluating with soft voting across 5 folds...\n",
            "\n",
            "Fold 1: Accuracy = 0.6750, Runtime = 20.63 seconds\n",
            "Fold 2: Accuracy = 0.6893, Runtime = 23.08 seconds\n",
            "Fold 3: Accuracy = 0.7214, Runtime = 25.32 seconds\n",
            "Fold 4: Accuracy = 0.7107, Runtime = 20.09 seconds\n",
            "Fold 5: Accuracy = 0.6714, Runtime = 24.69 seconds\n",
            "\n",
            "Average Accuracy (soft voting): 0.6936\n",
            "Average Fold Runtime: 22.76 seconds\n",
            "Total Runtime: 113.83 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks"
      ],
      "metadata": {
        "id": "Hn7yw4pHgFn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Network (FFNN)"
      ],
      "metadata": {
        "id": "jExbBJHbogCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to import tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Define vectorizers\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
        "    token_pattern=r'\\w{1,}', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
        "    lowercase=True\n",
        ")\n",
        "count_vectorizer = CountVectorizer(lowercase=True, max_features=None)\n",
        "\n",
        "# Choose vectorizer\n",
        "vectorizer = tfidf_vectorizer  # You can switch to count_vectorizer if desired\n",
        "\n",
        "# Vectorize the text data\n",
        "X = vectorizer.fit_transform(train_body).toarray()\n",
        "X_test = vectorizer.transform(test_body).toarray()\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(subreddit)\n",
        "\n",
        "# Define the dense neural network model\n",
        "def create_dnn(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),  # Define input shape with Input layer\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')  # Output layer with softmax for multi-class classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer and categorical crossentropy loss for multi-class classification\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "fold_times = []\n",
        "\n",
        "print(\"Performing K-fold cross-validation on the dense neural network...\\n\")\n",
        "overall_start = time.time()\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "    fold_start = time.time()\n",
        "\n",
        "    # Split data into training and validation sets for this fold\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Initialize the model\n",
        "    model = create_dnn(input_dim=X_train.shape[1], num_classes=len(np.unique(y)))\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    val_predictions = model.predict(X_val)\n",
        "    val_predictions = np.argmax(val_predictions, axis=1)\n",
        "    fold_accuracy = accuracy_score(y_val, val_predictions)\n",
        "\n",
        "    # Record accuracy and runtime\n",
        "    fold_end = time.time()\n",
        "    fold_runtime = fold_end - fold_start\n",
        "    fold_accuracies.append(fold_accuracy)\n",
        "    fold_times.append(fold_runtime)\n",
        "\n",
        "    print(f\"Fold {fold}: Accuracy = {fold_accuracy:.4f}, Runtime = {fold_runtime:.2f} seconds\")\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy = np.mean(fold_accuracies)\n",
        "avg_runtime = np.mean(fold_times)\n",
        "overall_runtime = time.time() - overall_start\n",
        "\n",
        "print(f\"\\nAverage Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Fold Runtime: {avg_runtime:.2f} seconds\")\n",
        "print(f\"Total Runtime: {overall_runtime:.2f} seconds\\n\")\n",
        "\n",
        "# Final training on all training data for test prediction\n",
        "final_model = create_dnn(input_dim=X.shape[1], num_classes=len(np.unique(y)))\n",
        "final_model.fit(\n",
        "    X, y,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Predict on the test data\n",
        "test_predictions = final_model.predict(X_test)\n",
        "test_predictions_labels = label_encoder.inverse_transform(np.argmax(test_predictions, axis=1))\n",
        "\n",
        "# Save final test predictions\n",
        "results_test_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions_labels})\n",
        "results_test_df.to_csv(\"final_predictions_dnn_test.csv\", index=False)\n",
        "print(\"\\nFinal test predictions saved to final_predictions_dnn_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NofydUjSgTVt",
        "outputId": "9269018f-502d-4d4d-a707-fd79d00b0238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing K-fold cross-validation on the dense neural network...\n",
            "\n",
            "Epoch 1/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.2525 - loss: 1.3843 - val_accuracy: 0.2500 - val_loss: 1.3793\n",
            "Epoch 2/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3601 - loss: 1.3695 - val_accuracy: 0.3000 - val_loss: 1.3612\n",
            "Epoch 3/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3780 - loss: 1.3410 - val_accuracy: 0.3929 - val_loss: 1.3228\n",
            "Epoch 4/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4618 - loss: 1.2856 - val_accuracy: 0.4321 - val_loss: 1.2634\n",
            "Epoch 5/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5186 - loss: 1.2131 - val_accuracy: 0.4321 - val_loss: 1.2098\n",
            "Epoch 6/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5635 - loss: 1.1460 - val_accuracy: 0.4821 - val_loss: 1.1653\n",
            "Epoch 7/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6484 - loss: 1.0531 - val_accuracy: 0.5143 - val_loss: 1.1229\n",
            "Epoch 8/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6997 - loss: 0.9595 - val_accuracy: 0.5643 - val_loss: 1.0689\n",
            "Epoch 9/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7788 - loss: 0.8472 - val_accuracy: 0.5821 - val_loss: 1.0139\n",
            "Epoch 10/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8445 - loss: 0.7079 - val_accuracy: 0.6429 - val_loss: 0.9536\n",
            "Epoch 11/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8819 - loss: 0.5743 - val_accuracy: 0.6393 - val_loss: 0.8989\n",
            "Epoch 12/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9362 - loss: 0.4408 - val_accuracy: 0.6500 - val_loss: 0.8552\n",
            "Epoch 13/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9582 - loss: 0.3407 - val_accuracy: 0.6607 - val_loss: 0.8332\n",
            "Epoch 14/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9806 - loss: 0.2386 - val_accuracy: 0.6679 - val_loss: 0.8203\n",
            "Epoch 15/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.1863 - val_accuracy: 0.6643 - val_loss: 0.8134\n",
            "Epoch 16/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9912 - loss: 0.1423 - val_accuracy: 0.6643 - val_loss: 0.8136\n",
            "Epoch 17/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0986 - val_accuracy: 0.6679 - val_loss: 0.8211\n",
            "Epoch 18/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9941 - loss: 0.0859 - val_accuracy: 0.6750 - val_loss: 0.8279\n",
            "Epoch 19/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0782 - val_accuracy: 0.6679 - val_loss: 0.8399\n",
            "Epoch 20/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0652 - val_accuracy: 0.6786 - val_loss: 0.8422\n",
            "Epoch 21/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0458 - val_accuracy: 0.6893 - val_loss: 0.8562\n",
            "Epoch 22/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0425 - val_accuracy: 0.6679 - val_loss: 0.8660\n",
            "Epoch 23/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0382 - val_accuracy: 0.6893 - val_loss: 0.8725\n",
            "Epoch 24/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 0.6893 - val_loss: 0.8836\n",
            "Epoch 25/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9990 - loss: 0.0296 - val_accuracy: 0.6893 - val_loss: 0.8955\n",
            "Epoch 26/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0244 - val_accuracy: 0.6929 - val_loss: 0.9050\n",
            "Epoch 27/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9960 - loss: 0.0306 - val_accuracy: 0.6893 - val_loss: 0.9067\n",
            "Epoch 28/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0226 - val_accuracy: 0.6714 - val_loss: 0.9276\n",
            "Epoch 29/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.6929 - val_loss: 0.9358\n",
            "Epoch 30/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0189 - val_accuracy: 0.6893 - val_loss: 0.9443\n",
            "Epoch 31/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0172 - val_accuracy: 0.6893 - val_loss: 0.9604\n",
            "Epoch 32/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0122 - val_accuracy: 0.6893 - val_loss: 0.9688\n",
            "Epoch 33/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0126 - val_accuracy: 0.6929 - val_loss: 0.9769\n",
            "Epoch 34/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.6929 - val_loss: 0.9927\n",
            "Epoch 35/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.6893 - val_loss: 0.9875\n",
            "Epoch 36/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.6964 - val_loss: 1.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.6893 - val_loss: 1.0140\n",
            "Epoch 38/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6893 - val_loss: 1.0117\n",
            "Epoch 39/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.6929 - val_loss: 1.0206\n",
            "Epoch 40/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.6893 - val_loss: 1.0221\n",
            "Epoch 41/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.6893 - val_loss: 1.0288\n",
            "Epoch 42/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.6964 - val_loss: 1.0421\n",
            "Epoch 43/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.6929 - val_loss: 1.0404\n",
            "Epoch 44/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6929 - val_loss: 1.0502\n",
            "Epoch 45/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.6857 - val_loss: 1.0746\n",
            "Epoch 46/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.6857 - val_loss: 1.0778\n",
            "Epoch 47/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.6857 - val_loss: 1.0815\n",
            "Epoch 48/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6786 - val_loss: 1.0802\n",
            "Epoch 49/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.6857 - val_loss: 1.0895\n",
            "Epoch 50/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 0.6786 - val_loss: 1.1006\n",
            "Epoch 51/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.6786 - val_loss: 1.1276\n",
            "Epoch 52/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6786 - val_loss: 1.1267\n",
            "Epoch 53/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.6714 - val_loss: 1.1376\n",
            "Epoch 54/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.6714 - val_loss: 1.1510\n",
            "Epoch 55/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.6714 - val_loss: 1.1617\n",
            "Epoch 56/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6714 - val_loss: 1.1766\n",
            "Epoch 57/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.6750 - val_loss: 1.1655\n",
            "Epoch 58/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6714 - val_loss: 1.1757\n",
            "Epoch 59/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.6750 - val_loss: 1.1860\n",
            "Epoch 60/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.6679 - val_loss: 1.2040\n",
            "Epoch 61/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6750 - val_loss: 1.1981\n",
            "Epoch 62/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.6643 - val_loss: 1.2112\n",
            "Epoch 63/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6679 - val_loss: 1.2280\n",
            "Epoch 64/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.6679 - val_loss: 1.2079\n",
            "Epoch 65/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.6714 - val_loss: 1.2076\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Fold 1: Accuracy = 0.6643, Runtime = 29.86 seconds\n",
            "Epoch 1/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.2272 - loss: 1.3866 - val_accuracy: 0.3607 - val_loss: 1.3836\n",
            "Epoch 2/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3737 - loss: 1.3792 - val_accuracy: 0.4143 - val_loss: 1.3755\n",
            "Epoch 3/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4502 - loss: 1.3667 - val_accuracy: 0.4393 - val_loss: 1.3567\n",
            "Epoch 4/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4876 - loss: 1.3396 - val_accuracy: 0.4357 - val_loss: 1.3195\n",
            "Epoch 5/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5266 - loss: 1.2878 - val_accuracy: 0.4357 - val_loss: 1.2565\n",
            "Epoch 6/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5192 - loss: 1.2046 - val_accuracy: 0.4929 - val_loss: 1.1874\n",
            "Epoch 7/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5760 - loss: 1.1156 - val_accuracy: 0.5214 - val_loss: 1.1259\n",
            "Epoch 8/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6848 - loss: 0.9823 - val_accuracy: 0.5536 - val_loss: 1.0684\n",
            "Epoch 9/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7682 - loss: 0.8600 - val_accuracy: 0.5786 - val_loss: 1.0089\n",
            "Epoch 10/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8283 - loss: 0.7270 - val_accuracy: 0.5929 - val_loss: 0.9530\n",
            "Epoch 11/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8943 - loss: 0.5635 - val_accuracy: 0.6536 - val_loss: 0.8954\n",
            "Epoch 12/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.4372 - val_accuracy: 0.6357 - val_loss: 0.8596\n",
            "Epoch 13/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.3192 - val_accuracy: 0.6429 - val_loss: 0.8386\n",
            "Epoch 14/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9910 - loss: 0.2259 - val_accuracy: 0.6536 - val_loss: 0.8639\n",
            "Epoch 15/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9911 - loss: 0.1684 - val_accuracy: 0.6429 - val_loss: 0.8440\n",
            "Epoch 16/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.1349 - val_accuracy: 0.6536 - val_loss: 0.8548\n",
            "Epoch 17/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.1124 - val_accuracy: 0.6571 - val_loss: 0.8703\n",
            "Epoch 18/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9923 - loss: 0.0831 - val_accuracy: 0.6571 - val_loss: 0.8863\n",
            "Epoch 19/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9907 - loss: 0.0696 - val_accuracy: 0.6500 - val_loss: 0.9077\n",
            "Epoch 20/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0624 - val_accuracy: 0.6536 - val_loss: 0.9218\n",
            "Epoch 21/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9972 - loss: 0.0466 - val_accuracy: 0.6607 - val_loss: 0.9318\n",
            "Epoch 22/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0366 - val_accuracy: 0.6643 - val_loss: 0.9554\n",
            "Epoch 23/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0283 - val_accuracy: 0.6571 - val_loss: 0.9823\n",
            "Epoch 24/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0274 - val_accuracy: 0.6607 - val_loss: 0.9953\n",
            "Epoch 25/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0250 - val_accuracy: 0.6643 - val_loss: 0.9844\n",
            "Epoch 26/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0206 - val_accuracy: 0.6393 - val_loss: 1.0259\n",
            "Epoch 27/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0187 - val_accuracy: 0.6571 - val_loss: 1.0297\n",
            "Epoch 28/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0164 - val_accuracy: 0.6607 - val_loss: 1.0463\n",
            "Epoch 29/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9980 - loss: 0.0161 - val_accuracy: 0.6679 - val_loss: 1.0351\n",
            "Epoch 30/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0136 - val_accuracy: 0.6607 - val_loss: 1.0695\n",
            "Epoch 31/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.6536 - val_loss: 1.0795\n",
            "Epoch 32/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.6607 - val_loss: 1.0877\n",
            "Epoch 33/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0113 - val_accuracy: 0.6607 - val_loss: 1.1067\n",
            "Epoch 34/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6679 - val_loss: 1.1185\n",
            "Epoch 35/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6607 - val_loss: 1.1547\n",
            "Epoch 36/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0093 - val_accuracy: 0.6607 - val_loss: 1.1863\n",
            "Epoch 37/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.6571 - val_loss: 1.1928\n",
            "Epoch 38/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.6464 - val_loss: 1.1905\n",
            "Epoch 39/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9991 - loss: 0.0064 - val_accuracy: 0.6429 - val_loss: 1.2059\n",
            "Epoch 40/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.6643 - val_loss: 1.1721\n",
            "Epoch 41/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6607 - val_loss: 1.1885\n",
            "Epoch 42/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6536 - val_loss: 1.2133\n",
            "Epoch 43/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.6679 - val_loss: 1.1973\n",
            "Epoch 44/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9992 - loss: 0.0069 - val_accuracy: 0.6679 - val_loss: 1.2043\n",
            "Epoch 45/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.6536 - val_loss: 1.2283\n",
            "Epoch 46/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.6607 - val_loss: 1.2421\n",
            "Epoch 47/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.6679 - val_loss: 1.2424\n",
            "Epoch 48/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6500 - val_loss: 1.2720\n",
            "Epoch 49/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.6536 - val_loss: 1.2878\n",
            "Epoch 50/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6429 - val_loss: 1.2852\n",
            "Epoch 51/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.6571 - val_loss: 1.3062\n",
            "Epoch 52/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.6607 - val_loss: 1.3184\n",
            "Epoch 53/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6571 - val_loss: 1.2975\n",
            "Epoch 54/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.6536 - val_loss: 1.3027\n",
            "Epoch 55/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.6536 - val_loss: 1.2991\n",
            "Epoch 56/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6571 - val_loss: 1.3429\n",
            "Epoch 57/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6536 - val_loss: 1.3498\n",
            "Epoch 58/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6643 - val_loss: 1.3147\n",
            "Epoch 59/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6607 - val_loss: 1.3479\n",
            "Epoch 60/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.6500 - val_loss: 1.3548\n",
            "Epoch 61/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6571 - val_loss: 1.3849\n",
            "Epoch 62/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.6571 - val_loss: 1.3891\n",
            "Epoch 63/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.6607 - val_loss: 1.3868\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Fold 2: Accuracy = 0.6429, Runtime = 20.48 seconds\n",
            "Epoch 1/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.2724 - loss: 1.3853 - val_accuracy: 0.3286 - val_loss: 1.3802\n",
            "Epoch 2/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3678 - loss: 1.3724 - val_accuracy: 0.4000 - val_loss: 1.3676\n",
            "Epoch 3/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4386 - loss: 1.3531 - val_accuracy: 0.3929 - val_loss: 1.3422\n",
            "Epoch 4/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4892 - loss: 1.3072 - val_accuracy: 0.4000 - val_loss: 1.2967\n",
            "Epoch 5/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4981 - loss: 1.2443 - val_accuracy: 0.4321 - val_loss: 1.2326\n",
            "Epoch 6/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5555 - loss: 1.1431 - val_accuracy: 0.4929 - val_loss: 1.1608\n",
            "Epoch 7/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5919 - loss: 1.0475 - val_accuracy: 0.5536 - val_loss: 1.1024\n",
            "Epoch 8/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7013 - loss: 0.9107 - val_accuracy: 0.6036 - val_loss: 1.0322\n",
            "Epoch 9/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8032 - loss: 0.7913 - val_accuracy: 0.6393 - val_loss: 0.9707\n",
            "Epoch 10/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8432 - loss: 0.6628 - val_accuracy: 0.6464 - val_loss: 0.9166\n",
            "Epoch 11/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9049 - loss: 0.5314 - val_accuracy: 0.6643 - val_loss: 0.8696\n",
            "Epoch 12/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9151 - loss: 0.4404 - val_accuracy: 0.6607 - val_loss: 0.8365\n",
            "Epoch 13/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9686 - loss: 0.3145 - val_accuracy: 0.6464 - val_loss: 0.8035\n",
            "Epoch 14/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.2202 - val_accuracy: 0.6536 - val_loss: 0.7804\n",
            "Epoch 15/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9912 - loss: 0.1661 - val_accuracy: 0.6643 - val_loss: 0.7667\n",
            "Epoch 16/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9925 - loss: 0.1353 - val_accuracy: 0.6393 - val_loss: 0.7916\n",
            "Epoch 17/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.1081 - val_accuracy: 0.6429 - val_loss: 0.7759\n",
            "Epoch 18/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9969 - loss: 0.0787 - val_accuracy: 0.6607 - val_loss: 0.7745\n",
            "Epoch 19/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0701 - val_accuracy: 0.6500 - val_loss: 0.7954\n",
            "Epoch 20/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0455 - val_accuracy: 0.6500 - val_loss: 0.7972\n",
            "Epoch 21/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9996 - loss: 0.0400 - val_accuracy: 0.6464 - val_loss: 0.8024\n",
            "Epoch 22/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0328 - val_accuracy: 0.6607 - val_loss: 0.8195\n",
            "Epoch 23/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0297 - val_accuracy: 0.6464 - val_loss: 0.8413\n",
            "Epoch 24/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0279 - val_accuracy: 0.6571 - val_loss: 0.8394\n",
            "Epoch 25/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0240 - val_accuracy: 0.6464 - val_loss: 0.8421\n",
            "Epoch 26/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0204 - val_accuracy: 0.6500 - val_loss: 0.8487\n",
            "Epoch 27/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0219 - val_accuracy: 0.6643 - val_loss: 0.8541\n",
            "Epoch 28/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0167 - val_accuracy: 0.6500 - val_loss: 0.8784\n",
            "Epoch 29/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0174 - val_accuracy: 0.6571 - val_loss: 0.8778\n",
            "Epoch 30/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.0143 - val_accuracy: 0.6500 - val_loss: 0.8957\n",
            "Epoch 31/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0117 - val_accuracy: 0.6536 - val_loss: 0.9085\n",
            "Epoch 32/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0120 - val_accuracy: 0.6464 - val_loss: 0.9033\n",
            "Epoch 33/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.6536 - val_loss: 0.9248\n",
            "Epoch 34/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.6536 - val_loss: 0.9293\n",
            "Epoch 35/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6571 - val_loss: 0.9144\n",
            "Epoch 36/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.6607 - val_loss: 0.9428\n",
            "Epoch 37/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0070 - val_accuracy: 0.6607 - val_loss: 0.9482\n",
            "Epoch 38/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0064 - val_accuracy: 0.6571 - val_loss: 0.9629\n",
            "Epoch 39/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.6536 - val_loss: 0.9757\n",
            "Epoch 40/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.6643 - val_loss: 0.9582\n",
            "Epoch 41/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.6714 - val_loss: 0.9578\n",
            "Epoch 42/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0068 - val_accuracy: 0.6607 - val_loss: 0.9848\n",
            "Epoch 43/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.6571 - val_loss: 0.9936\n",
            "Epoch 44/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6536 - val_loss: 0.9982\n",
            "Epoch 45/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.6500 - val_loss: 1.0048\n",
            "Epoch 46/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.6393 - val_loss: 1.0178\n",
            "Epoch 47/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.6500 - val_loss: 1.0218\n",
            "Epoch 48/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6464 - val_loss: 1.0288\n",
            "Epoch 49/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.6500 - val_loss: 1.0315\n",
            "Epoch 50/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.6536 - val_loss: 1.0368\n",
            "Epoch 51/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.6393 - val_loss: 1.0417\n",
            "Epoch 52/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.6500 - val_loss: 1.0523\n",
            "Epoch 53/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.6500 - val_loss: 1.0502\n",
            "Epoch 54/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.6500 - val_loss: 1.0616\n",
            "Epoch 55/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.6500 - val_loss: 1.0614\n",
            "Epoch 56/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6536 - val_loss: 1.0629\n",
            "Epoch 57/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.6536 - val_loss: 1.0784\n",
            "Epoch 58/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.6500 - val_loss: 1.0916\n",
            "Epoch 59/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.6500 - val_loss: 1.0883\n",
            "Epoch 60/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.6571 - val_loss: 1.0902\n",
            "Epoch 61/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.6536 - val_loss: 1.0983\n",
            "Epoch 62/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6393 - val_loss: 1.1234\n",
            "Epoch 63/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6464 - val_loss: 1.1220\n",
            "Epoch 64/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6429 - val_loss: 1.1342\n",
            "Epoch 65/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.6536 - val_loss: 1.1228\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Fold 3: Accuracy = 0.6643, Runtime = 22.64 seconds\n",
            "Epoch 1/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2648 - loss: 1.3847 - val_accuracy: 0.4357 - val_loss: 1.3791\n",
            "Epoch 2/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4247 - loss: 1.3747 - val_accuracy: 0.4821 - val_loss: 1.3676\n",
            "Epoch 3/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4641 - loss: 1.3590 - val_accuracy: 0.5000 - val_loss: 1.3460\n",
            "Epoch 4/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5120 - loss: 1.3290 - val_accuracy: 0.5071 - val_loss: 1.3073\n",
            "Epoch 5/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5320 - loss: 1.2766 - val_accuracy: 0.5214 - val_loss: 1.2456\n",
            "Epoch 6/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6107 - loss: 1.1881 - val_accuracy: 0.5607 - val_loss: 1.1695\n",
            "Epoch 7/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6605 - loss: 1.0800 - val_accuracy: 0.6286 - val_loss: 1.0880\n",
            "Epoch 8/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7709 - loss: 0.9336 - val_accuracy: 0.6571 - val_loss: 1.0138\n",
            "Epoch 9/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8324 - loss: 0.8057 - val_accuracy: 0.6679 - val_loss: 0.9316\n",
            "Epoch 10/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9025 - loss: 0.6277 - val_accuracy: 0.6643 - val_loss: 0.8634\n",
            "Epoch 11/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9514 - loss: 0.4596 - val_accuracy: 0.6786 - val_loss: 0.8224\n",
            "Epoch 12/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9688 - loss: 0.3686 - val_accuracy: 0.6964 - val_loss: 0.7688\n",
            "Epoch 13/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 0.2490 - val_accuracy: 0.7143 - val_loss: 0.7443\n",
            "Epoch 14/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9900 - loss: 0.1823 - val_accuracy: 0.7179 - val_loss: 0.7346\n",
            "Epoch 15/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9921 - loss: 0.1427 - val_accuracy: 0.7107 - val_loss: 0.7308\n",
            "Epoch 16/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.1197 - val_accuracy: 0.7179 - val_loss: 0.7374\n",
            "Epoch 17/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9941 - loss: 0.0959 - val_accuracy: 0.7214 - val_loss: 0.7338\n",
            "Epoch 18/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0714 - val_accuracy: 0.7107 - val_loss: 0.7447\n",
            "Epoch 19/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9990 - loss: 0.0536 - val_accuracy: 0.7143 - val_loss: 0.7506\n",
            "Epoch 20/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0494 - val_accuracy: 0.7250 - val_loss: 0.7661\n",
            "Epoch 21/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0421 - val_accuracy: 0.7179 - val_loss: 0.7702\n",
            "Epoch 22/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0399 - val_accuracy: 0.7143 - val_loss: 0.7749\n",
            "Epoch 23/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0305 - val_accuracy: 0.7214 - val_loss: 0.7880\n",
            "Epoch 24/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0276 - val_accuracy: 0.7107 - val_loss: 0.7971\n",
            "Epoch 25/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 0.0235 - val_accuracy: 0.7000 - val_loss: 0.8035\n",
            "Epoch 26/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0214 - val_accuracy: 0.7071 - val_loss: 0.8065\n",
            "Epoch 27/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0190 - val_accuracy: 0.7107 - val_loss: 0.8145\n",
            "Epoch 28/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0169 - val_accuracy: 0.7107 - val_loss: 0.8258\n",
            "Epoch 29/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.0176 - val_accuracy: 0.7107 - val_loss: 0.8367\n",
            "Epoch 30/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 0.7143 - val_loss: 0.8461\n",
            "Epoch 31/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 0.7107 - val_loss: 0.8458\n",
            "Epoch 32/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 0.7143 - val_loss: 0.8496\n",
            "Epoch 33/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 0.7179 - val_loss: 0.8617\n",
            "Epoch 34/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.7107 - val_loss: 0.8674\n",
            "Epoch 35/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.7143 - val_loss: 0.8783\n",
            "Epoch 36/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.7143 - val_loss: 0.8851\n",
            "Epoch 37/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.7143 - val_loss: 0.8896\n",
            "Epoch 38/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.7107 - val_loss: 0.8977\n",
            "Epoch 39/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.7179 - val_loss: 0.9019\n",
            "Epoch 40/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.7143 - val_loss: 0.9082\n",
            "Epoch 41/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.7107 - val_loss: 0.9171\n",
            "Epoch 42/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.7179 - val_loss: 0.9227\n",
            "Epoch 43/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7107 - val_loss: 0.9275\n",
            "Epoch 44/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7143 - val_loss: 0.9364\n",
            "Epoch 45/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7214 - val_loss: 0.9436\n",
            "Epoch 46/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.7071 - val_loss: 0.9528\n",
            "Epoch 47/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7071 - val_loss: 0.9524\n",
            "Epoch 48/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.7179 - val_loss: 0.9593\n",
            "Epoch 49/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.7071 - val_loss: 0.9671\n",
            "Epoch 50/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.7143 - val_loss: 0.9656\n",
            "Epoch 51/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7179 - val_loss: 0.9705\n",
            "Epoch 52/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.7179 - val_loss: 0.9714\n",
            "Epoch 53/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.7036 - val_loss: 0.9825\n",
            "Epoch 54/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6929 - val_loss: 0.9808\n",
            "Epoch 55/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7000 - val_loss: 0.9849\n",
            "Epoch 56/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7000 - val_loss: 0.9924\n",
            "Epoch 57/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7036 - val_loss: 0.9993\n",
            "Epoch 58/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7000 - val_loss: 1.0080\n",
            "Epoch 59/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7107 - val_loss: 1.0066\n",
            "Epoch 60/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7107 - val_loss: 1.0103\n",
            "Epoch 61/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.7071 - val_loss: 1.0225\n",
            "Epoch 62/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7071 - val_loss: 1.0299\n",
            "Epoch 63/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.7036 - val_loss: 1.0353\n",
            "Epoch 64/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.7036 - val_loss: 1.0313\n",
            "Epoch 65/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.7071 - val_loss: 1.0362\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Fold 4: Accuracy = 0.7107, Runtime = 22.85 seconds\n",
            "Epoch 1/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.2414 - loss: 1.3854 - val_accuracy: 0.3179 - val_loss: 1.3801\n",
            "Epoch 2/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3505 - loss: 1.3767 - val_accuracy: 0.4143 - val_loss: 1.3687\n",
            "Epoch 3/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4544 - loss: 1.3598 - val_accuracy: 0.4714 - val_loss: 1.3442\n",
            "Epoch 4/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5031 - loss: 1.3295 - val_accuracy: 0.5571 - val_loss: 1.2977\n",
            "Epoch 5/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5431 - loss: 1.2718 - val_accuracy: 0.5143 - val_loss: 1.2284\n",
            "Epoch 6/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5942 - loss: 1.1791 - val_accuracy: 0.5214 - val_loss: 1.1538\n",
            "Epoch 7/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6333 - loss: 1.0914 - val_accuracy: 0.5286 - val_loss: 1.0947\n",
            "Epoch 8/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6797 - loss: 0.9776 - val_accuracy: 0.5571 - val_loss: 1.0376\n",
            "Epoch 9/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7656 - loss: 0.8667 - val_accuracy: 0.6071 - val_loss: 0.9740\n",
            "Epoch 10/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8033 - loss: 0.7146 - val_accuracy: 0.6536 - val_loss: 0.9152\n",
            "Epoch 11/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8948 - loss: 0.5630 - val_accuracy: 0.6429 - val_loss: 0.8652\n",
            "Epoch 12/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9389 - loss: 0.4350 - val_accuracy: 0.6571 - val_loss: 0.8264\n",
            "Epoch 13/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9547 - loss: 0.3243 - val_accuracy: 0.6607 - val_loss: 0.8018\n",
            "Epoch 14/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9769 - loss: 0.2380 - val_accuracy: 0.6679 - val_loss: 0.7978\n",
            "Epoch 15/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9832 - loss: 0.1897 - val_accuracy: 0.6643 - val_loss: 0.7917\n",
            "Epoch 16/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9910 - loss: 0.1403 - val_accuracy: 0.6714 - val_loss: 0.7927\n",
            "Epoch 17/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9981 - loss: 0.1051 - val_accuracy: 0.6714 - val_loss: 0.8027\n",
            "Epoch 18/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9871 - loss: 0.0957 - val_accuracy: 0.6893 - val_loss: 0.8061\n",
            "Epoch 19/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9950 - loss: 0.0750 - val_accuracy: 0.6607 - val_loss: 0.8294\n",
            "Epoch 20/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0566 - val_accuracy: 0.6607 - val_loss: 0.8448\n",
            "Epoch 21/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9962 - loss: 0.0541 - val_accuracy: 0.6786 - val_loss: 0.8497\n",
            "Epoch 22/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9987 - loss: 0.0407 - val_accuracy: 0.6750 - val_loss: 0.8535\n",
            "Epoch 23/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0333 - val_accuracy: 0.6821 - val_loss: 0.8634\n",
            "Epoch 24/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0332 - val_accuracy: 0.6786 - val_loss: 0.8868\n",
            "Epoch 25/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9975 - loss: 0.0278 - val_accuracy: 0.6893 - val_loss: 0.8741\n",
            "Epoch 26/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0233 - val_accuracy: 0.6750 - val_loss: 0.9209\n",
            "Epoch 27/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0256 - val_accuracy: 0.6786 - val_loss: 0.9196\n",
            "Epoch 28/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0190 - val_accuracy: 0.6786 - val_loss: 0.9358\n",
            "Epoch 29/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9998 - loss: 0.0174 - val_accuracy: 0.6893 - val_loss: 0.9298\n",
            "Epoch 30/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0135 - val_accuracy: 0.6714 - val_loss: 0.9521\n",
            "Epoch 31/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0152 - val_accuracy: 0.6679 - val_loss: 0.9708\n",
            "Epoch 32/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 0.6857 - val_loss: 0.9614\n",
            "Epoch 33/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 0.6679 - val_loss: 0.9815\n",
            "Epoch 34/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 0.6643 - val_loss: 0.9946\n",
            "Epoch 35/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0105 - val_accuracy: 0.6643 - val_loss: 1.0025\n",
            "Epoch 36/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.6714 - val_loss: 1.0121\n",
            "Epoch 37/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9990 - loss: 0.0096 - val_accuracy: 0.6643 - val_loss: 1.0214\n",
            "Epoch 38/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.6643 - val_loss: 1.0335\n",
            "Epoch 39/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.6607 - val_loss: 1.0505\n",
            "Epoch 40/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.6679 - val_loss: 1.0452\n",
            "Epoch 41/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 0.6643 - val_loss: 1.0429\n",
            "Epoch 42/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0073 - val_accuracy: 0.6643 - val_loss: 1.0694\n",
            "Epoch 43/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.6679 - val_loss: 1.0702\n",
            "Epoch 44/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.6679 - val_loss: 1.0912\n",
            "Epoch 45/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6679 - val_loss: 1.0958\n",
            "Epoch 46/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.6714 - val_loss: 1.1070\n",
            "Epoch 47/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0046 - val_accuracy: 0.6679 - val_loss: 1.1170\n",
            "Epoch 48/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.6679 - val_loss: 1.1098\n",
            "Epoch 49/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.6714 - val_loss: 1.1255\n",
            "Epoch 50/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6679 - val_loss: 1.1441\n",
            "Epoch 51/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.6679 - val_loss: 1.1391\n",
            "Epoch 52/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.6679 - val_loss: 1.1472\n",
            "Epoch 53/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.6750 - val_loss: 1.1542\n",
            "Epoch 54/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.6714 - val_loss: 1.1619\n",
            "Epoch 55/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.6714 - val_loss: 1.1679\n",
            "Epoch 56/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.6714 - val_loss: 1.1716\n",
            "Epoch 57/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.6786 - val_loss: 1.1751\n",
            "Epoch 58/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.6786 - val_loss: 1.1846\n",
            "Epoch 59/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.6821 - val_loss: 1.1901\n",
            "Epoch 60/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.6786 - val_loss: 1.1969\n",
            "Epoch 61/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6714 - val_loss: 1.1973\n",
            "Epoch 62/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0044 - val_accuracy: 0.6714 - val_loss: 1.1917\n",
            "Epoch 63/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6786 - val_loss: 1.2042\n",
            "Epoch 64/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6750 - val_loss: 1.2177\n",
            "Epoch 65/100\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.6821 - val_loss: 1.2036\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Fold 5: Accuracy = 0.6643, Runtime = 23.37 seconds\n",
            "\n",
            "Average Accuracy: 0.6693\n",
            "Average Fold Runtime: 23.84 seconds\n",
            "Total Runtime: 119.21 seconds\n",
            "\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\n",
            "Final test predictions saved to final_predictions_dnn_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "qPbkv14Hoklh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "# Define vectorizers\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    min_df=2, max_features=None, strip_accents='unicode', analyzer='word',\n",
        "    token_pattern=r'\\w{1,}', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
        "    lowercase=True\n",
        ")\n",
        "count_vectorizer = CountVectorizer(lowercase=True, max_features=None)\n",
        "\n",
        "# Choose vectorizer\n",
        "vectorizer = tfidf_vectorizer  # You can switch to count_vectorizer if desired\n",
        "\n",
        "# Vectorize the text data\n",
        "X = vectorizer.fit_transform(train_body).toarray()\n",
        "X_test = vectorizer.transform(test_body).toarray()\n",
        "# Converts the sparse matrix from TF-IDF vectorization into a dense array (a regular NumPy array).\n",
        "# In case of using Sklearn, we can skip it because Sklearn works well with sparse matrix. But TensorFlow does not!\n",
        "\n",
        "# Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(subreddit)\n",
        "\n",
        "# Define the function to create the DNN with customizable hyperparameters\n",
        "def create_dnn(num_classes, layer_sizes, dropout_rate):\n",
        "    model = Sequential([\n",
        "        Input(shape=(X.shape[1],)),\n",
        "        Dense(layer_sizes[0], activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(layer_sizes[1], activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(layer_sizes[2], activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Best parameters: {'batch_size': 32, 'dropout_rate': 0.4, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'layer_sizes': [(512, 256, 128), (256, 128, 64), (128, 64, 32)],\n",
        "    'dropout_rate': [0.2, 0.5],\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "\n",
        "# Initialize variables to store the best hyperparameters and accuracy\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "print(\"Hyperparameter tuning...\\n\")\n",
        "\n",
        "# Iterate over each combination of hyperparameters\n",
        "for params in ParameterGrid(param_grid):\n",
        "    learning_rate = params['learning_rate']\n",
        "    layer_sizes = params['layer_sizes']\n",
        "    dropout_rate = params['dropout_rate']\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    print(f\"Testing parameters: {params}\")\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(X), 1):\n",
        "        # Split data into training and validation sets for this fold\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Initialize the model with the current set of hyperparameters\n",
        "        model = create_dnn(num_classes=len(np.unique(y)), layer_sizes=layer_sizes, dropout_rate=dropout_rate)\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Early stopping to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=50,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate on validation data\n",
        "        val_predictions = model.predict(X_val)\n",
        "        val_predictions = np.argmax(val_predictions, axis=1)\n",
        "        fold_accuracy = accuracy_score(y_val, val_predictions)\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "    # Calculate average accuracy for this parameter set\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    print(f\"Parameters: {params}, Average Accuracy = {avg_accuracy:.4f}\")\n",
        "\n",
        "    # Update best parameters based on accuracy\n",
        "    if avg_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_accuracy\n",
        "        best_params = params\n",
        "\n",
        "print(f\"\\nBest parameters: {best_params} with average accuracy: {best_accuracy:.4f}\\n\")\n",
        "\n",
        "# Final training on all training data for test prediction with the best parameters\n",
        "final_model = create_dnn(\n",
        "    num_classes=len(np.unique(y)),\n",
        "    layer_sizes=best_params['layer_sizes'],\n",
        "    dropout_rate=best_params['dropout_rate']\n",
        ")\n",
        "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping for final model training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "# Train the model on the full training dataset with the best parameters\n",
        "final_model.fit(\n",
        "    X, y,\n",
        "    epochs=50,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Predict on the test data\n",
        "test_predictions = final_model.predict(X_test)\n",
        "test_predictions_labels = np.argmax(test_predictions, axis=1)  # Get the predicted class indices\n",
        "\n",
        "# Calculate and print accuracy on test data (only if y_test is available)\n",
        "# Uncomment the following lines if you have y_test\n",
        "# test_accuracy = accuracy_score(y_test, test_predictions_labels)\n",
        "# print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Convert predictions back to original labels if needed\n",
        "test_predictions_labels = label_encoder.inverse_transform(test_predictions_labels)\n",
        "\n",
        "# Save final test predictions\n",
        "results_test_df = pd.DataFrame({'id': id_test, 'subreddit': test_predictions_labels})\n",
        "results_test_df.to_csv(\"final_predictions_dnn_test.csv\", index=False)\n",
        "print(\"\\nFinal test predictions saved to final_predictions_dnn_test.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duZKdW-buxKJ",
        "outputId": "75e36470-f0f9-4b34-c7fe-5ad89c0b5f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter tuning...\n",
            "\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}, Average Accuracy = 0.6643\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}, Average Accuracy = 0.6693\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}, Average Accuracy = 0.6300\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}, Average Accuracy = 0.6707\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}, Average Accuracy = 0.6521\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}, Average Accuracy = 0.6693\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}, Average Accuracy = 0.6222\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}, Average Accuracy = 0.6821\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}, Average Accuracy = 0.6443\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}, Average Accuracy = 0.6736\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}, Average Accuracy = 0.6722\n",
            "Testing parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}, Average Accuracy = 0.6543\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}, Average Accuracy = 0.6379\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}, Average Accuracy = 0.6672\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}, Average Accuracy = 0.6329\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}, Average Accuracy = 0.6607\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}, Average Accuracy = 0.6714\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.2, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}, Average Accuracy = 0.6721\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.001}, Average Accuracy = 0.6514\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001}, Average Accuracy = 0.6807\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.001}, Average Accuracy = 0.6764\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (256, 128, 64), 'learning_rate': 0.0001}, Average Accuracy = 0.6593\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.001}, Average Accuracy = 0.6586\n",
            "Testing parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Parameters: {'batch_size': 64, 'dropout_rate': 0.5, 'layer_sizes': (128, 64, 32), 'learning_rate': 0.0001}, Average Accuracy = 0.6229\n",
            "\n",
            "Best parameters: {'batch_size': 32, 'dropout_rate': 0.5, 'layer_sizes': (512, 256, 128), 'learning_rate': 0.0001} with average accuracy: 0.6821\n",
            "\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\n",
            "Final test predictions saved to final_predictions_dnn_test.csv\n"
          ]
        }
      ]
    }
  ]
}